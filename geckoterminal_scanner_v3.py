"""
GeckoTerminal Scanner V3 - OPTIMIS√â avec enseignements Backtest Phase 2
Am√©liorations bas√©es sur analyse de 3,261 alertes historiques:

NOUVEAUT√âS V3 (Win Rate attendu: 35-50% vs 18.9% actuel):
‚úÖ Arbitrum D√âSACTIV√â (4.9% WR catastrophique)
‚úÖ Base optimis√© (seuils augment√©s: $300K liq, $1M vol)
‚úÖ Filtre v√©locit√© minimum >5 (Facteur #1: +133% impact)
‚úÖ Filtre type pump (rejeter LENT: 73% des √©checs)
‚úÖ Filtre √¢ge token optimis√© (favoriser 2-3 jours: 36.1% WR)
‚úÖ Syst√®me de TIERS (HIGH/MEDIUM/LOW confidence)
‚úÖ Liquidit√© optimale par r√©seau
‚úÖ Watchlist automatique (snowball, RTX, TTD, FIREBALL)
‚úÖ Scoring dynamique am√©lior√©

Fonctionnalit√©s V2 conserv√©es:
- Multi-pool correlation
- Momentum multi-timeframe
- Traders spike detection
- Buy/Sell pressure evolution
- Alertes ACCELERATION
- R√©sistance/Support detection
"""

import sys
import os
import time
import json
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from collections import defaultdict

# Syst√®me de s√©curit√© et tracking
from security_checker import SecurityChecker
from alert_tracker import AlertTracker

# UTF-8 pour emojis Windows
if sys.platform == "win32":
    import io
    if hasattr(sys.stdout, 'buffer'):
        try:
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        except (ValueError, AttributeError):
            pass

# ============================================
# CONFIGURATION V3 - CANAL TELEGRAM S√âPAR√â
# ============================================

# Charger variables d'environnement depuis .env.v3 (si disponible)
try:
    from dotenv import load_dotenv
    # Charger .env.v3 en priorit√©, sinon .env par d√©faut
    env_v3_path = os.path.join(os.path.dirname(__file__), '.env.v3')
    if os.path.exists(env_v3_path):
        load_dotenv(env_v3_path)
        print("üîß V3: Configuration charg√©e depuis .env.v3")
    else:
        load_dotenv()
        print("‚ö†Ô∏è V3: .env.v3 non trouv√©, utilisation .env par d√©faut")
except ImportError:
    print("‚ö†Ô∏è V3: python-dotenv non install√©, variables syst√®me utilis√©es")

GECKOTERMINAL_API = "https://api.geckoterminal.com/api/v2"

# Configuration Telegram V3 (peut √™tre diff√©rente de V2 si .env.v3 utilis√©)
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "")

# Afficher configuration au d√©marrage
if TELEGRAM_CHAT_ID and TELEGRAM_BOT_TOKEN:
    print(f"‚úÖ V3 Telegram configur√©: Chat ID = {TELEGRAM_CHAT_ID}")
    print(f"   Bot Token: {TELEGRAM_BOT_TOKEN[:20]}...")
else:
    print(f"‚ö†Ô∏è WARNING: Telegram NON configur√©!")
    print(f"   TELEGRAM_BOT_TOKEN: {'‚úÖ OK' if TELEGRAM_BOT_TOKEN else '‚ùå MANQUANT'}")
    print(f"   TELEGRAM_CHAT_ID: {'‚úÖ OK' if TELEGRAM_CHAT_ID else '‚ùå MANQUANT'}")

# R√©seaux √† surveiller - V3.1: ARBITRUM D√âSACTIV√â (4.4% quality rate)
# V3.2: Ajout Polygon et Avalanche pour augmenter volume d'opportunit√©s
# Polygon: Tr√®s actif memecoins, frais bas
# Avalanche: √âcosyst√®me DeFi mature, bonne qualit√©
NETWORKS = ["eth", "bsc", "base", "solana", "polygon_pos", "avax"]  # V3.2: +Polygon +Avalanche

# ============================================
# SEUILS PAR R√âSEAU - Configuration centralis√©e
# ============================================

# Fonction pour construire NETWORK_THRESHOLDS selon le mode actif
def build_network_thresholds(mode_config):
    """Construit NETWORK_THRESHOLDS avec limites de liquidit√© du mode actif."""
    liq = mode_config['LIQUIDITY']
    return {
        "solana": {
            "min_liquidity": liq['solana'][0],
            "max_liquidity": liq['solana'][1],
            "min_volume": 50000,
            "min_txns": 100
        },
        "bsc": {
            "min_liquidity": liq['bsc'][0],
            "max_liquidity": liq['bsc'][1],
            "min_volume": 100000,
            "min_txns": 100
        },
        "eth": {
            "min_liquidity": liq['eth'][0],
            "max_liquidity": liq['eth'][1],
            "min_volume": 50000,
            "min_txns": 100
        },
        "base": {
            "min_liquidity": liq['base'][0],
            "max_liquidity": liq['base'][1],
            "min_volume": 1000000,
            "min_txns": 150
        },
        "arbitrum": {
            "min_liquidity": 100000,
            "max_liquidity": 1000000,
            "min_volume": 50000,
            "min_txns": 100
        },
        "avax": {
            "min_liquidity": liq.get('avax', (100000, 800000))[0],
            "max_liquidity": liq.get('avax', (100000, 800000))[1],
            "min_volume": 50000,
            "min_txns": 100
        },
        "polygon_pos": {
            "min_liquidity": liq.get('polygon_pos', (50000, 500000))[0],
            "max_liquidity": liq.get('polygon_pos', (50000, 500000))[1],
            "min_volume": 30000,  # Plus bas car frais tr√®s bas sur Polygon
            "min_txns": 80
        },
        "default": {
            "min_liquidity": 100000,
            "min_volume": 50000,
            "min_txns": 100
        }
    }

# ============================================
# V3.1: CONFIGURATION ULTRA_RENTABLE - Qualit√© maximale (2.7 alertes/jour)
# ============================================
# Objectif: 2.7 alertes/jour | Score 95.9 | WR 55-70% | ROI +10-15%/mois
# Bas√© sur analyse de 4252 alertes Railway

print("=" * 80)
print("V3.2 DASHBOARD - POLYGON + AVALANCHE - 2025-12-30 23:00")
print("Objectif: 8-10 alertes/jour | Score 91.4 | WR 45-58% | ROI +4-7%/mois")
print("NEW: +Polygon +Avalanche | Fallback FDV/MarketCap liquidity")
print("=" * 80)

# Configuration DASHBOARD (5 alertes/jour)
DASHBOARD_CONFIG = {
    'MIN_VELOCITE_PUMP': 5.0,
    'NETWORK_SCORE_FILTERS': {
        'eth': {'min_score': 78, 'min_velocity': 5},
        'base': {'min_score': 82, 'min_velocity': 8},
        'bsc': {'min_score': 80, 'min_velocity': 6},
        'solana': {'min_score': 72, 'min_velocity': 5},
        'polygon_pos': {'min_score': 75, 'min_velocity': 5},  # V3.2: Moins strict car frais bas
        'avax': {'min_score': 80, 'min_velocity': 6},  # V3.2: Similar √† BSC
    },
    'LIQUIDITY': {
        'eth': (80000, 600000),
        'base': (250000, 2500000),
        'bsc': (400000, 6000000),
        'solana': (80000, 300000),
        'polygon_pos': (50000, 500000),  # V3.2: Plus bas car frais tr√®s bas
        'avax': (100000, 800000),  # V3.2: Similar √† ETH
    }
}

# Appliquer la configuration
MIN_VELOCITE_PUMP = DASHBOARD_CONFIG['MIN_VELOCITE_PUMP']
NETWORK_SCORE_FILTERS = DASHBOARD_CONFIG['NETWORK_SCORE_FILTERS']
NETWORK_THRESHOLDS = build_network_thresholds(DASHBOARD_CONFIG)

# ============================================
# V3: NOUVEAUX FILTRES (Backtest Phase 2)
# ============================================

OPTIMAL_VELOCITE_PUMP = 30.0     # Bonus si > 30
EXPLOSIVE_VELOCITE_PUMP = 50.0   # Bonus suppl√©mentaire si > 50

# Filtre TYPE PUMP (73% des losers sont "LENT")
ALLOWED_PUMP_TYPES = ["RAPIDE", "TRES_RAPIDE", "PARABOLIQUE"]  # Rejeter LENT, STAGNANT
REJECTED_PUMP_TYPES = ["LENT", "STAGNANT", "STABLE"]

# Filtre √ÇGE TOKEN - V3.1 STRAT√âGIE HYBRIDE
# Analyse 4252 alertes:
# - Zone EMBRYONIC 0-3h: Quality Index 182.83 (MEILLEUR!)
# - Zone DANGER 12-24h: Quality Index 36.87 (PIRE)
# - Zone MATURE 48-72h: Win Rate 36.1% (stable)
# V3.1: Accepter 0-3h (embryonic) + 48-72h (mature), √©viter 12-24h
MIN_TOKEN_AGE_HOURS = 0.0        # V3.1: CRITIQUE - Accepter embryonic 0-3h
EMBRYONIC_AGE_MAX_HOURS = 3.0    # Zone embryonic: 0-3h (QI 182.83)
OPTIMAL_TOKEN_AGE_MIN_HOURS = 48.0   # Zone mature: 48-72h (WR 36.1%)
OPTIMAL_TOKEN_AGE_MAX_HOURS = 72.0
MAX_TOKEN_AGE_HOURS = 168.0      # Max 7 jours
DANGER_ZONE_AGE_MIN = 12.0       # √âviter zone danger 12-24h
DANGER_ZONE_AGE_MAX = 24.0       # Quality Index 36.87 (PIRE)

# Watchlist automatique (tokens "Money Printer")
# snowball/SOL: 100% WR sur 81 alertes!
# RTX/USDT: 100% WR sur 20 alertes
# TTD/USDT: 77.8% WR sur 45 alertes
# FIREBALL/SOL: 77.4% WR sur 31 alertes
WATCHLIST_TOKENS = [
    "snowball", "RTX", "TTD", "FIREBALL",
    # Ajouter d'autres tokens avec historique exceptionnel
]

# Seuils globaux (h√©rit√©s, conserv√©s pour compatibilit√©)
VOLUME_LIQUIDITY_RATIO = 0.5    # Vol24h/Liquidit√© > 50%

# Seuils pour signaux avanc√©s
TRADERS_SPIKE_THRESHOLD = 0.5   # +50% traders
BUY_RATIO_THRESHOLD = 0.8       # 80% buy ratio
BUY_RATIO_CHANGE_THRESHOLD = 0.15  # +15% variation
ACCELERATION_THRESHOLD = 0.05   # +5% en 1h
VOLUME_SPIKE_THRESHOLD = 0.5    # +50% volume

# Cooldown et limites
COOLDOWN_SECONDS = 0  # D√âSACTIV√â pour backtesting - collecte toutes les occurrences
MAX_ALERTS_PER_SCAN = 10  # Augment√© de 5 √† 10 pour collecte max

# NOUVEAU: Param√®tres de re-alerting intelligent (Bug #1 fix)
MIN_PRICE_CHANGE_PERCENT = 5.0  # Re-alerter si variation ¬±5% depuis entry
MIN_TIME_HOURS_FOR_REALERT = 4.0  # Re-alerter apr√®s 4h m√™me sans changement
ENABLE_SMART_REALERT = False  # D√âSACTIV√â pour phase backtesting (collecte max de donn√©es)

# TRACKING ACTIF: Param√®tres pour suivre les pools alert√©s (BACKTESTING)
ENABLE_ACTIVE_TRACKING = True  # Activer le tracking actif des pools alert√©s
ACTIVE_TRACKING_MAX_AGE_HOURS = 24  # Suivre les alertes des derni√®res 24h
ACTIVE_TRACKING_UPDATE_COOLDOWN_MINUTES = 15  # Cooldown 15min entre mises √† jour

# ============================================
# CACHE SIMPLIFI√â
# ============================================
# On garde seulement buy_ratio history (pas fourni par API)
buy_ratio_history = defaultdict(lambda: defaultdict(list))

# Multi-pool tracking
token_pools = defaultdict(list)  # [base_token] = [pool_data, pool_data, ...]
alert_cooldown = {}

# Syst√®me de s√©curit√© et tracking (initialis√©s dans main())
security_checker = None
alert_tracker = None

# ============================================
# UTILITAIRES
# ============================================
# Mapping des networks pour affichage lisible
NETWORK_NAMES = {
    "eth": "Ethereum",
    "bsc": "BSC (Binance Smart Chain)",
    "arbitrum": "Arbitrum",
    "base": "Base",
    "solana": "Solana",
    "polygon_pos": "Polygon",
    "avax": "Avalanche",
    "optimism": "Optimism",
    "fantom": "Fantom",
}

def get_network_display_name(network_id: str) -> str:
    """Convertit ID network en nom lisible."""
    return NETWORK_NAMES.get(network_id.lower(), network_id.upper())

def log(msg: str):
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {msg}")

def send_telegram(message: str) -> bool:
    """Envoie alerte Telegram."""
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
        data = {
            "chat_id": TELEGRAM_CHAT_ID,
            "text": message,
            "parse_mode": "Markdown",
            "disable_web_page_preview": True
        }
        response = requests.post(url, json=data, timeout=10)
        return response.status_code == 200
    except Exception as e:
        log(f"‚ùå Erreur Telegram: {e}")
        return False

def extract_base_token(pool_name: str) -> str:
    """Extrait le nom du base token depuis le nom du pool."""
    # Ex: "LAVA / USDT 0.01%" -> "LAVA"
    if "/" in pool_name:
        return pool_name.split("/")[0].strip()
    return pool_name.strip()

def check_cooldown(alert_key: str) -> bool:
    """V√©rifie si alerte en cooldown (LEGACY - utiliser should_send_alert √† la place)."""
    now = time.time()
    if alert_key in alert_cooldown:
        elapsed = now - alert_cooldown[alert_key]
        if elapsed < COOLDOWN_SECONDS:
            return False
    alert_cooldown[alert_key] = now
    return True


def should_send_alert(token_address: str, current_price: float, tracker, regle5_data: Dict = None) -> Tuple[bool, str]:
    """
    D√©termine si une alerte doit √™tre envoy√©e pour un token (FIX BUG #1 - SPAM).

    Logique intelligente:
    - 1√®re alerte: TOUJOURS envoyer
    - Alertes suivantes: SEULEMENT si:
        * TP atteint (TP1/TP2/TP3) OU
        * Prix a vari√© de ¬±5% depuis entry OU
        * 4h se sont √©coul√©es depuis derni√®re alerte OU
        * Pump parabolique d√©tect√© (v√©locit√© >100%/h)

    Returns:
        (should_send: bool, reason: str)
    """
    # V√©rifier si c'est la premi√®re alerte pour ce token
    if not tracker.token_already_alerted(token_address):
        return True, "Premi√®re alerte pour ce token"

    # Si syst√®me intelligent d√©sactiv√©, toujours envoyer
    if not ENABLE_SMART_REALERT:
        return True, "Smart re-alert d√©sactiv√©"

    # R√©cup√©rer la derni√®re alerte pour ce token
    previous_alert = tracker.get_last_alert_for_token(token_address)
    if not previous_alert:
        return True, "Pas d'alerte pr√©c√©dente trouv√©e"

    # 1. V√©rifier si un TP a √©t√© atteint
    entry_price = previous_alert.get('entry_price', 0)
    tp1_price = previous_alert.get('tp1_price', 0)
    tp2_price = previous_alert.get('tp2_price', 0)
    tp3_price = previous_alert.get('tp3_price', 0)

    # R√©cup√©rer le prix MAX atteint (pas seulement le prix actuel)
    alert_id = previous_alert.get('id', 0)
    prix_max_atteint = current_price
    if alert_id > 0:
        prix_max_db = tracker.get_highest_price_for_alert(alert_id)
        if prix_max_db:
            prix_max_atteint = max(prix_max_db, current_price)

    # FIX HARMONISATION: Tol√©rance 0.5% pour coh√©rence avec analyser_alerte_suivante()
    TP_TOLERANCE_PERCENT = 0.5

    def tp_reached_with_tolerance(prix: float, tp_target: float) -> bool:
        """V√©rifie si TP atteint avec tol√©rance pour arrondi."""
        if tp_target <= 0:
            return False
        ecart_percent = ((prix - tp_target) / tp_target) * 100
        return ecart_percent >= -TP_TOLERANCE_PERCENT

    if tp_reached_with_tolerance(prix_max_atteint, tp1_price):
        return True, f"TP atteint (prix max: ${prix_max_atteint:.6f} >= TP1: ${tp1_price:.6f})"

    # 2. V√©rifier si le prix a vari√© de ¬±5% depuis entry
    if entry_price > 0:
        price_change_pct = abs((current_price - entry_price) / entry_price * 100)
        if price_change_pct >= MIN_PRICE_CHANGE_PERCENT:
            return True, f"Variation prix significative: {price_change_pct:.1f}% depuis entry"

    # 3. V√©rifier le temps √©coul√© depuis la derni√®re alerte
    created_at_str = previous_alert.get('created_at', '')
    if created_at_str:
        try:
            from datetime import datetime
            created_at = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))
            now = datetime.now(created_at.tzinfo) if created_at.tzinfo else datetime.now()
            elapsed = (now - created_at).total_seconds() / 3600  # En heures

            if elapsed >= MIN_TIME_HOURS_FOR_REALERT:
                return True, f"Temps √©coul√© suffisant: {elapsed:.1f}h"
        except Exception as e:
            log(f"‚ö†Ô∏è Erreur parsing date: {e}")

    # 4. V√©rifier si pump parabolique (R√àGLE 5)
    if regle5_data and regle5_data.get('type_pump') == 'PARABOLIQUE':
        return True, f"Pump PARABOLIQUE d√©tect√© - Alerte SORTIR urgente"

    # Aucune raison de re-alerter ‚Üí SPAM PREVENTION
    return False, f"Pas de changement significatif (prix: ${current_price:.6f}, entry: ${entry_price:.6f})"

# ============================================
# API CALLS
# ============================================
def get_trending_pools(network: str, page: int = 1) -> Optional[List[Dict]]:
    """R√©cup√®re pools trending sur un r√©seau."""
    try:
        url = f"{GECKOTERMINAL_API}/networks/{network}/trending_pools"
        params = {"page": page}
        headers = {"Accept": "application/json"}
        response = requests.get(url, params=params, headers=headers, timeout=15)

        if response.status_code == 429:
            log(f"‚ö†Ô∏è Rate limit atteint, pause 60s...")
            time.sleep(60)
            return None
        if response.status_code != 200:
            log(f"‚ö†Ô∏è Erreur {network}: {response.status_code}")
            return None

        data = response.json()
        pools = data.get("data", [])

        # DEBUG: Log premier pool brut pour diagnostic
        if pools and page == 1 and not hasattr(get_trending_pools, '_logged_raw'):
            import json as json_lib
            log(f"   [DEBUG-RAW-POOL] Premier trending pool {network}:")
            log(f"      {json_lib.dumps(pools[0], indent=2)[:800]}")  # 800 premiers caract√®res
            get_trending_pools._logged_raw = True

        return pools
    except Exception as e:
        log(f"‚ùå Erreur get_trending_pools {network}: {e}")
        return None

def get_new_pools(network: str, page: int = 1) -> Optional[List[Dict]]:
    """R√©cup√®re nouveaux pools sur un r√©seau."""
    try:
        url = f"{GECKOTERMINAL_API}/networks/{network}/new_pools"
        params = {"page": page}
        headers = {"Accept": "application/json"}
        response = requests.get(url, params=params, headers=headers, timeout=15)

        if response.status_code == 429:
            log(f"‚ö†Ô∏è Rate limit atteint, pause 60s...")
            time.sleep(60)
            return None
        if response.status_code != 200:
            return None

        data = response.json()
        return data.get("data", [])
    except Exception as e:
        log(f"‚ùå Erreur get_new_pools {network}: {e}")
        return None

def get_pool_by_address(network: str, pool_address: str) -> Optional[Dict]:
    """
    R√©cup√®re les donn√©es d'un pool sp√©cifique via son adresse.
    Utilis√© pour le tracking actif des alertes.

    Args:
        network: R√©seau (eth, bsc, solana, etc.)
        pool_address: Adresse du pool

    Returns:
        Dict avec les donn√©es du pool, ou None si erreur
    """
    try:
        url = f"{GECKOTERMINAL_API}/networks/{network}/pools/{pool_address}"
        headers = {"Accept": "application/json"}
        response = requests.get(url, headers=headers, timeout=15)

        if response.status_code == 429:
            log(f"‚ö†Ô∏è Rate limit atteint pour pool {pool_address[:8]}...")
            time.sleep(60)
            return None
        if response.status_code != 200:
            log(f"‚ö†Ô∏è Pool {pool_address[:8]} non trouv√© (status {response.status_code})")
            return None

        data = response.json()
        pool_data = data.get("data")

        if pool_data:
            return parse_pool_data(pool_data, network)
        return None

    except Exception as e:
        log(f"‚ùå Erreur get_pool_by_address {pool_address[:8]}: {e}")
        return None

# ============================================
# PARSING & ENRICHISSEMENT
# ============================================
def parse_pool_data(pool: Dict, network: str = "unknown") -> Optional[Dict]:
    """Parse donn√©es pool GeckoTerminal avec enrichissements."""
    try:
        attrs = pool.get("attributes", {})

        # DEBUG: Log structure du premier pool pour diagnostiquer
        if not hasattr(parse_pool_data, '_logged_structure'):
            log(f"   [DEBUG-STRUCTURE] Premier pool re√ßu:")
            log(f"      Pool keys: {list(pool.keys())}")
            log(f"      Attributes keys: {list(attrs.keys())[:10]}")  # Premiers 10 keys
            if 'reserve_in_usd' in attrs:
                log(f"      reserve_in_usd found: {attrs['reserve_in_usd']}")
            else:
                log(f"      reserve_in_usd NOT FOUND in attributes!")
            parse_pool_data._logged_structure = True

        # Infos de base
        name = attrs.get("name", "Unknown")
        base_token_name = extract_base_token(name)
        base_token_price = attrs.get("base_token_price_usd")
        price_usd = float(base_token_price) if base_token_price else 0

        # Volume et liquidit√© (prot√©ger contre None)
        volume_usd_data = attrs.get("volume_usd", {}) or {}
        volume_24h = float(volume_usd_data.get("h24") or 0)
        volume_6h = float(volume_usd_data.get("h6") or 0)
        volume_1h = float(volume_usd_data.get("h1") or 0)

        # Liquidity - avec fallback sur FDV et Market Cap
        reserve_value = attrs.get("reserve_in_usd")
        liquidity = 0
        liquidity_source = "none"

        # Essayer reserve_in_usd d'abord
        if reserve_value and reserve_value not in [None, "", "0.0", "0"]:
            try:
                liquidity = float(reserve_value)
                if liquidity > 0:
                    liquidity_source = "reserve_in_usd"
            except (ValueError, TypeError):
                liquidity = 0

        # FALLBACK 1: Si reserve_in_usd est 0, essayer fdv_usd (10% du FDV)
        if liquidity == 0:
            fdv_value = attrs.get("fdv_usd")
            if fdv_value and fdv_value not in [None, "", "0.0", "0", "null"]:
                try:
                    fdv = float(fdv_value)
                    if fdv > 0:
                        # Estimer liquidit√© √† 10% du FDV (conservateur)
                        liquidity = fdv * 0.10
                        liquidity_source = "fdv_usd(10%)"
                except (ValueError, TypeError):
                    pass

        # FALLBACK 2: Si toujours 0, essayer market_cap_usd (15% du market cap)
        if liquidity == 0:
            mcap_value = attrs.get("market_cap_usd")
            if mcap_value and mcap_value not in [None, "", "0.0", "0", "null"]:
                try:
                    mcap = float(mcap_value)
                    if mcap > 0:
                        # Estimer liquidit√© √† 15% du market cap
                        liquidity = mcap * 0.15
                        liquidity_source = "market_cap(15%)"
                except (ValueError, TypeError):
                    pass

        # Log pour debug
        if liquidity == 0:
            log(f"   [LIQUIDITY] {name}: FAILED - reserve={reserve_value}, fdv={attrs.get('fdv_usd')}, mcap={attrs.get('market_cap_usd')}")
        elif liquidity_source != "reserve_in_usd":
            log(f"   [LIQUIDITY] {name}: ${liquidity:,.0f} from {liquidity_source}")

        # Transactions (prot√©ger contre None)
        transactions_data = attrs.get("transactions", {}) or {}
        txns_24h = transactions_data.get("h24", {}) or {}
        txns_6h = transactions_data.get("h6", {}) or {}
        txns_1h = transactions_data.get("h1", {}) or {}

        buys_24h = txns_24h.get("buys", 0)
        sells_24h = txns_24h.get("sells", 0)
        buys_6h = txns_6h.get("buys", 0)
        sells_6h = txns_6h.get("sells", 0)
        buys_1h = txns_1h.get("buys", 0)
        sells_1h = txns_1h.get("sells", 0)

        # NOUVEAU: Wallets uniques (buyers/sellers) - FEATURE WHALE DETECTION
        buyers_24h = txns_24h.get("buyers", 0)
        sellers_24h = txns_24h.get("sellers", 0)
        buyers_6h = txns_6h.get("buyers", 0)
        sellers_6h = txns_6h.get("sellers", 0)
        buyers_1h = txns_1h.get("buyers", 0)
        sellers_1h = txns_1h.get("sellers", 0)

        total_txns = buys_24h + sells_24h

        # Variations prix (multi-timeframe depuis API) - Prot√©ger contre None
        price_changes = attrs.get("price_change_percentage", {}) or {}
        price_change_24h = float(price_changes.get("h24") or 0)
        price_change_6h = float(price_changes.get("h6") or 0)
        price_change_3h = float(price_changes.get("h3") or 0)
        price_change_1h = float(price_changes.get("h1") or 0)

        # Age du pool
        pool_created = attrs.get("pool_created_at")
        if pool_created:
            created_dt = datetime.fromisoformat(pool_created.replace('Z', '+00:00'))
            age_hours = (datetime.now().astimezone() - created_dt).total_seconds() / 3600
        else:
            age_hours = 999999

        # R√©seau et adresse - Utiliser le param√®tre network pass√© directement
        # On garde le network pass√© en param√®tre (fourni lors de l'appel de l'API)
        # Si ce n'√©tait pas fourni, on essaie de l'extraire des relationships
        if network == "unknown":
            relationships = pool.get("relationships", {}) or {}
            network_data = relationships.get("network", {}) or {}
            network_inner = network_data.get("data", {}) or {}
            network = network_inner.get("id", "unknown")

            # Si network toujours unknown, essayer de l'extraire du type
            if network == "unknown":
                pool_type = pool.get("type", "")
                # Format peut √™tre "pool" ou "network-name-pool"
                if "-" in pool_type:
                    network = pool_type.split("-")[0]

        pool_address = attrs.get("address", "")

        # FDV et Market Cap (prot√©ger contre None)
        fdv_usd = float(attrs.get("fdv_usd") or 0)
        market_cap_usd = float(attrs.get("market_cap_usd") or 0)

        # Calculer l'acc√©l√©ration du volume (NOUVEAU)
        vol_24h_avg = volume_24h / 24 if volume_24h > 0 else 0
        vol_6h_avg = volume_6h / 6 if volume_6h > 0 else 0

        volume_acceleration_1h_vs_6h = (volume_1h / vol_6h_avg) if vol_6h_avg > 0 else 0
        volume_acceleration_6h_vs_24h = (vol_6h_avg / vol_24h_avg) if vol_24h_avg > 0 else 0

        # ===== V3: CALCULER VELOCITE ET TYPE PUMP (depuis price_change_1h) =====
        # Utiliser price_change_1h comme proxy de v√©locit√© (% par heure)
        velocite_pump = abs(price_change_1h) if price_change_1h else 0

        # D√©terminer type de pump bas√© sur v√©locit√©
        if velocite_pump > 100:
            type_pump = "PARABOLIQUE"
        elif velocite_pump > 50:
            type_pump = "TRES_RAPIDE"
        elif velocite_pump > 20:
            type_pump = "RAPIDE"
        elif velocite_pump > 5:
            type_pump = "NORMAL"
        else:
            type_pump = "LENT"

        # Ajouter token_name et token_symbol pour watchlist check
        token_name = base_token_name  # D√©j√† extrait
        token_symbol = base_token_name  # On utilise le nom de base comme symbole

        return {
            "name": name,
            "base_token_name": base_token_name,
            "token_name": token_name,  # V3: Pour watchlist
            "token_symbol": token_symbol,  # V3: Pour watchlist
            "price_usd": price_usd,
            "volume_24h": volume_24h,
            "volume_6h": volume_6h,
            "volume_1h": volume_1h,
            "liquidity": liquidity,
            "total_txns": total_txns,
            "buys_24h": buys_24h,
            "sells_24h": sells_24h,
            "buys_6h": buys_6h,
            "sells_6h": sells_6h,
            "buys_1h": buys_1h,
            "sells_1h": sells_1h,
            "buyers_24h": buyers_24h,
            "sellers_24h": sellers_24h,
            "buyers_6h": buyers_6h,
            "sellers_6h": sellers_6h,
            "buyers_1h": buyers_1h,
            "sellers_1h": sellers_1h,
            "price_change_24h": price_change_24h,
            "price_change_6h": price_change_6h,
            "price_change_3h": price_change_3h,
            "price_change_1h": price_change_1h,
            "age_hours": age_hours,
            "network": network,
            "pool_address": pool_address,
            "fdv_usd": fdv_usd,
            "market_cap_usd": market_cap_usd,
            "volume_acceleration_1h_vs_6h": volume_acceleration_1h_vs_6h,
            "volume_acceleration_6h_vs_24h": volume_acceleration_6h_vs_24h,
            "velocite_pump": velocite_pump,  # V3: Pour filtres backtest
            "type_pump": type_pump,  # V3: Pour filtres backtest
        }
    except Exception as e:
        log(f"‚ö†Ô∏è Erreur parse pool: {e}")
        return None

# ============================================
# HISTORIQUE BUY RATIO (SIMPLIFI√â)
# ============================================
def update_buy_ratio_history(pool_data: Dict):
    """Met √† jour seulement l'historique buy ratio (pas fourni par API)."""
    base_token = pool_data["base_token_name"]
    pool_addr = pool_data["pool_address"]
    now = time.time()

    # Buy ratio 24h
    buy_ratio = pool_data["buys_24h"] / pool_data["sells_24h"] if pool_data["sells_24h"] > 0 else 1.0
    buy_ratio_history[base_token][pool_addr].append((now, buy_ratio))

    # Nettoyer historique (garder 2h seulement - on a besoin que de 1h)
    cutoff = now - 7200  # 2h
    buy_ratio_history[base_token][pool_addr] = [
        (t, v) for t, v in buy_ratio_history[base_token][pool_addr] if t > cutoff
    ]

def get_buy_ratio_change(base_token: str, pool_addr: str) -> Optional[float]:
    """Calcule variation buy ratio sur 1h."""
    hist = buy_ratio_history[base_token][pool_addr]

    if not hist or len(hist) < 2:
        return None

    now = time.time()
    one_hour_ago = now - 3600  # 1h

    # Trouver valeur la plus proche d'il y a 1h
    past_values = [v for t, v in hist if t < one_hour_ago]
    if not past_values:
        return None

    past = past_values[-1]  # Plus r√©cente valeur avant 1h
    current = hist[-1][1]

    if past == 0:
        return None

    return ((current - past) / past) * 100

def get_price_momentum_from_api(pool_data: Dict) -> Dict[str, Optional[float]]:
    """
    SIMPLIFI√â: Utilise directement les donn√©es de l'API au lieu de recalculer.
    GeckoTerminal fournit d√©j√† price_change_1h, price_change_3h, price_change_6h !
    """
    return {
        "1h": pool_data.get("price_change_1h"),
        "3h": pool_data.get("price_change_3h"),
        "6h": pool_data.get("price_change_6h"),
    }

def find_resistance_simple(pool_data: Dict) -> Dict:
    """
    SIMPLIFI√â: R√©sistance bas√©e sur prix actuel et variation.
    Pour une vraie r√©sistance, il faudrait un historique long-terme.
    """
    # R√©sistance simple = prix actuel + 10% (estimation conservative)
    current_price = pool_data["price_usd"]

    # Si prix en hausse forte (>20% sur 24h), r√©sistance proche
    if pool_data["price_change_24h"] > 20:
        resistance = current_price * 1.05  # +5% seulement
    else:
        resistance = current_price * 1.10  # +10%

    resistance_dist_pct = ((resistance - current_price) / current_price) * 100

    return {
        "resistance": resistance,
        "support": None,  # Pas calcul√© pour simplifier
        "resistance_dist_pct": resistance_dist_pct,
    }

# ============================================
# MULTI-POOL CORRELATION
# ============================================
def group_pools_by_token(all_pools: List[Dict]) -> Dict[str, List[Dict]]:
    """Regroupe pools par base token."""
    grouped = defaultdict(list)
    for pool in all_pools:
        base_token = pool["base_token_name"]
        grouped[base_token].append(pool)
    return grouped

def analyze_multi_pool(pools: List[Dict]) -> Dict:
    """Analyse activit√© cross-pool d'un token."""
    if len(pools) <= 1:
        return {"is_multi_pool": False}

    total_volume = sum(p["volume_24h"] for p in pools)
    total_liquidity = sum(p["liquidity"] for p in pools)

    # Identifier pool dominant
    pools_sorted = sorted(pools, key=lambda x: x["volume_24h"], reverse=True)
    dominant_pool = pools_sorted[0]

    # Type de pool dominant (USDT, WETH, etc)
    dominant_pair = dominant_pool["name"].split("/")[1].strip().split()[0] if "/" in dominant_pool["name"] else "Unknown"

    # Vol/Liq par pool
    pool_activities = []
    for p in pools:
        vol_liq = (p["volume_24h"] / p["liquidity"] * 100) if p["liquidity"] > 0 else 0
        pair_name = p["name"].split("/")[1].strip().split()[0] if "/" in p["name"] else "Unknown"
        pool_activities.append({
            "pair": pair_name,
            "vol_liq_pct": vol_liq,
            "volume": p["volume_24h"],
        })

    # Signal bullish si WETH > USDT
    is_bullish = False
    weth_activity = next((p for p in pool_activities if "WETH" in p["pair"] or "ETH" in p["pair"]), None)
    usdt_activity = next((p for p in pool_activities if "USDT" in p["pair"] or "USDC" in p["pair"]), None)

    if weth_activity and usdt_activity:
        if weth_activity["vol_liq_pct"] > usdt_activity["vol_liq_pct"]:
            is_bullish = True

    return {
        "is_multi_pool": True,
        "num_pools": len(pools),
        "total_volume": total_volume,
        "total_liquidity": total_liquidity,
        "dominant_pair": dominant_pair,
        "pool_activities": pool_activities,
        "is_weth_dominant": is_bullish,
    }

# ============================================
# SCORING DYNAMIQUE
# ============================================
def calculate_base_score(pool_data: Dict) -> int:
    """
    Score de base DYNAMIQUE V3 (fondamentaux) avec p√©nalit√©s ET bonus backtest.
    Corrig√© pour ALMANAK: score ne doit plus monter pendant une chute.
    V3: Ajout bonus r√©seau, zones optimales liquidit√©, √¢ge optimal.
    """
    score = 0

    # === BONUS R√âSEAU V3 (max 35 points - nouveau!) ===
    network = pool_data.get("network", "")
    liq = pool_data.get("liquidity", 0)

    # DEBUG: Log si liquidit√© manquante
    if liq == 0:
        log(f"   [DEBUG] Pool {pool_data.get('name', 'UNK')}: liq=0, vol={pool_data.get('volume_24h', 0)}, age={pool_data.get('age_hours', 0)}")
        return 0  # Score 0 si pas de liquidit√©

    # ETH et Solana = champions (38.9% WR)
    if network == "eth":
        score += 35
        # BONUS ZONE JACKPOT ETH $100K-$200K (55.6% WR, +6,987% ROI!)
        if 100000 <= liq <= 200000:
            score += 15  # JACKPOT!
    elif network == "solana":
        score += 32
        # BONUS ZONE OPTIMALE Solana $100K-$200K (43.8% WR)
        if 100000 <= liq <= 200000:
            score += 12
    elif network == "bsc":
        score += 25  # 23.4% WR
        # BONUS ZONE OPTIMALE BSC $500K-$5M (36-39% WR)
        if 500000 <= liq <= 5000000:
            score += 10
    elif network == "base":
        score += 15  # 12.8% WR (faible)
    elif network == "polygon_pos":
        score += 20  # V3.2: Nouveau r√©seau, score mod√©r√© (√† ajuster)
        # Zone optimale Polygon: frais bas = plus d'activit√©
        if 50000 <= liq <= 300000:
            score += 10
    elif network == "avax":
        score += 28  # V3.2: √âcosyst√®me mature, similar √† BSC
        # Zone optimale Avalanche
        if 100000 <= liq <= 500000:
            score += 12
    elif network == "arbitrum":
        score += 5   # 4.9% WR (catastrophique)
    else:
        score += 10  # R√©seau inconnu (prudence)

    # Liquidit√© (max 25 points - r√©duit car r√©seau/zones d√©j√† compt√©s)
    if liq >= 1000000:
        score += 15  # R√©duit (gros tokens d√©j√† d√©couverts)
    elif liq >= 500000:
        score += 20
    elif liq >= 200000:
        score += 25  # SWEET SPOT g√©n√©ral
    elif liq >= 100000:
        score += 25  # SWEET SPOT g√©n√©ral
    elif liq >= 50000:
        score += 15

    # Volume (max 20 points)
    vol = pool_data["volume_24h"]
    if vol >= 1000000:
        score += 20
    elif vol >= 500000:
        score += 15
    elif vol >= 200000:
        score += 10
    elif vol >= 100000:
        score += 5

    # === √ÇGE V3 OPTIMIS√â (max 25 points - augment√©!) ===
    age = pool_data["age_hours"]
    if 48 <= age <= 72:  # 2-3 JOURS = OPTIMAL (36.1% WR!)
        score += 25  # MAXIMUM
    elif 24 <= age < 48 or 72 < age <= 96:  # Autour de l'optimal
        score += 20
    elif 6 <= age < 24:  # Acceptable
        score += 15
    elif age < 6:
        score += 8   # Tr√®s r√©cent (risqu√©, -34% drawdown)
    elif 96 < age <= 168:
        score += 10  # Une semaine (OK)
    # P√âNALIT√â ZONE DANGER 12-24h (8.6% WR!)
    if 12 <= age <= 24:
        score -= 15  # PIRE timing!

    # Vol/Liq ratio (max 15 points)
    vol_liq = (vol / liq) if liq > 0 else 0
    if 0.5 <= vol_liq <= 1.5:
        score += 15  # Sweet spot
    elif 0.3 <= vol_liq < 0.5 or 1.5 < vol_liq <= 2.0:
        score += 10
    elif vol_liq > 2.0:
        score += 5  # Trop actif (pump?)

    # Buy/Sell balance (max 15 points)
    buy_ratio = pool_data["buys_24h"] / pool_data["sells_24h"] if pool_data["sells_24h"] > 0 else 1.0
    if 0.6 <= buy_ratio <= 1.4:
        score += 15  # √âquilibr√©
    elif 0.4 <= buy_ratio < 0.6 or 1.4 < buy_ratio <= 2.0:
        score += 10
    elif buy_ratio < 0.4 or buy_ratio > 2.0:
        score += 5  # D√©s√©quilibr√©

    # === NOUVELLES P√âNALIT√âS DYNAMIQUES (Correction ALMANAK) ===

    # P√âNALIT√â 1: Tendance baissi√®re 24h (CRITIQUE)
    price_change_24h = pool_data.get("price_change_24h", 0)
    if price_change_24h < -40:
        score -= 35  # Crash massif (-45% comme ALMANAK)
    elif price_change_24h < -30:
        score -= 25  # Chute s√©v√®re
    elif price_change_24h < -20:
        score -= 15  # Chute importante
    elif price_change_24h < -10:
        score -= 8   # Baisse mod√©r√©e

    # P√âNALIT√â 2: Pression vendeuse massive 24h (CRITIQUE)
    total_txns_24h = pool_data["buys_24h"] + pool_data["sells_24h"]
    if total_txns_24h > 0:
        sell_pressure_24h = pool_data["sells_24h"] / total_txns_24h
        if sell_pressure_24h > 0.70:  # >70% ventes (cas ALMANAK: 63-68%)
            score -= 25
        elif sell_pressure_24h > 0.65:  # >65% ventes
            score -= 20
        elif sell_pressure_24h > 0.60:  # >60% ventes
            score -= 12

    # P√âNALIT√â 3: Liquidit√© en chute (risque rug pull)
    # Note: N√©cessiterait historique liquidit√©, simul√© via Vol/Liq ratio anormal
    if vol_liq > 3.0:  # Activit√© excessive = possiblement liquidit√© qui fond
        score -= 10

    return max(score, 0)  # Ne jamais descendre sous 0

def calculate_momentum_bonus(pool_data: Dict, momentum: Dict, multi_pool_data: Dict) -> int:
    """
    Bonus momentum INTELLIGENT V3 (dynamique actuelle).
    Corrig√© pour ALMANAK: distingue bon spike (accumulation) vs mauvais spike (panic sell).
    V3: Ajout bonus v√©locit√© (facteur #1 de succ√®s: +133% impact).
    """
    # VALIDATION: V√©rifier que momentum est un dict valide
    if not momentum or not isinstance(momentum, dict):
        log(f"   ‚ö†Ô∏è momentum invalide dans calculate_momentum_bonus: {type(momentum)}")
        return 0  # Pas de bonus si momentum invalide

    bonus = 0

    # === V√âLOCIT√â V3 (max 20 points - NOUVEAU! Facteur #1) ===
    velocite = pool_data.get('velocite_pump', 0)
    if velocite >= 100:  # PARABOLIQUE
        bonus += 20  # MAXIMUM
    elif velocite >= 50:  # TRES_RAPIDE (pattern gagnant!)
        bonus += 18
    elif velocite >= 20:  # RAPIDE
        bonus += 15
    elif velocite >= 10:
        bonus += 10
    elif velocite >= 5:
        bonus += 5
    # < 5 = d√©j√† filtr√© en amont, mais si passe quand m√™me:
    elif velocite < 5:
        bonus -= 5  # P√âNALIT√â

    # === Prix 1h (max 15 points - r√©duit car v√©locit√© d√©j√† compt√©) ===
    price_1h = momentum.get("1h", 0)
    price_6h = pool_data.get("price_change_6h", 0)
    price_24h = pool_data.get("price_change_24h", 0)

    if price_1h >= 10:
        bonus += 10  # R√©duit de 15 √† 10
    elif price_1h >= 5:
        bonus += 7   # R√©duit de 10 √† 7
    elif price_1h >= 2:
        bonus += 3   # R√©duit de 5 √† 3

    # === D√âTECTION DEAD CAT BOUNCE (Correction ALMANAK) ===
    # Rebond 1h positif MAIS tendance 24h tr√®s n√©gative = pi√®ge
    if price_1h > 0 and price_24h < -20:
        # Dead cat bounce d√©tect√©
        if price_1h < 10:  # Petit rebond sur grosse chute
            bonus -= 10  # P√âNALIT√â au lieu de bonus!
        # Si rebond >10%, on garde le bonus mais on avertira dans les signaux

    # === Volume Spike: BON ou MAUVAIS? ===
    txn_1h = pool_data["buys_1h"] + pool_data["sells_1h"]
    txn_24h = pool_data["total_txns"]

    if txn_24h > 0:
        txn_1h_normalized = txn_1h * 24  # Normaliser sur 24h
        has_volume_spike = txn_1h_normalized > txn_24h * 1.5  # +50% activit√©

        if has_volume_spike:
            # Spike + Prix hausse = BON (accumulation)
            if price_1h > 3:
                bonus += 10
            # Spike + Prix stable/l√©ger hausse = NEUTRE
            elif price_1h > 0:
                bonus += 5
            # Spike + Prix chute = MAUVAIS (panic sell)
            elif price_1h < -5:
                bonus -= 10  # P√âNALIT√â pour panic sell
            # Spike + Prix l√©ger baisse = vigilance
            else:
                bonus += 0  # Pas de bonus ni p√©nalit√©
        elif txn_1h_normalized > txn_24h * 1.2:
            # Activit√© mod√©r√©e
            if price_1h > 0:
                bonus += 5

    # === Buy Pressure 1h (max 10 points) ===
    buy_ratio_1h = pool_data["buys_1h"] / pool_data["sells_1h"] if pool_data["sells_1h"] > 0 else 1.0
    buy_ratio_24h = pool_data["buys_24h"] / pool_data["sells_24h"] if pool_data["sells_24h"] > 0 else 1.0

    # Buy ratio 1h DOIT √™tre compar√© au ratio 24h
    if buy_ratio_1h >= 1.2:  # Forte dominance acheteurs
        bonus += 10
    elif buy_ratio_1h >= 1.0:  # L√©ger avantage acheteurs
        bonus += 8
    elif buy_ratio_1h >= 0.8:  # √âquilibre
        bonus += 5
    elif buy_ratio_1h < 0.5 and buy_ratio_24h < 0.5:
        # Vendeurs dominent 1h ET 24h = tr√®s mauvais
        bonus -= 5

    # === Multi-pool (max 10 points) ===
    if multi_pool_data.get("is_multi_pool"):
        bonus += 5
        if multi_pool_data.get("is_weth_dominant"):
            bonus += 5

    # === CORRECTION CONTEXTE GLOBAL ===
    # Si price_24h tr√®s n√©gatif, limiter le momentum bonus max
    if price_24h < -30:
        bonus = min(bonus, 10)  # Max 10 points si token en crash
    elif price_24h < -20:
        bonus = min(bonus, 15)  # Max 15 points si token en chute

    return max(min(bonus, 30), -20)  # Max +30, Min -20

# ============================================
# WHALE ACTIVITY ANALYSIS (NOUVEAU)
# ============================================
def analyze_whale_activity(pool_data: Dict) -> Dict:
    """
    Analyse l'activit√© des whales via unique buyers/sellers.

    D√©tecte:
    - Whale manipulation (peu de wallets, beaucoup de txns)
    - Accumulation distribu√©e (beaucoup de wallets)
    - Sentiment du march√© (buyers vs sellers)

    Returns:
        {
            'pattern': str,              # WHALE_MANIPULATION / DISTRIBUTED_BUYING / WHALE_SELLING / NORMAL
            'whale_score': int,          # -20 √† +20 (bonus/malus au score)
            'avg_buys_per_buyer': float,
            'avg_sells_per_seller': float,
            'unique_wallet_ratio': float, # buyers / sellers
            'concentration_risk': str,    # LOW / MEDIUM / HIGH
            'signals': list               # Liste des signaux d√©tect√©s
        }
    """
    signals = []
    whale_score = 0

    # R√©cup√©rer les donn√©es 1h (plus r√©cent = plus important)
    # HOTFIX: G√©rer None explicite de l'API (or 0 = fallback)
    buys_1h = pool_data.get('buys_1h') or 0
    sells_1h = pool_data.get('sells_1h') or 0
    buyers_1h = pool_data.get('buyers_1h') or 0
    sellers_1h = pool_data.get('sellers_1h') or 0

    # R√©cup√©rer 24h pour contexte
    buys_24h = pool_data.get('buys_24h') or 0
    buyers_24h = pool_data.get('buyers_24h') or 0
    sellers_24h = pool_data.get('sellers_24h') or 0

    # Calculer moyennes de transactions par wallet unique
    avg_buys_per_buyer = buys_1h / buyers_1h if buyers_1h > 0 else 0
    avg_sells_per_seller = sells_1h / sellers_1h if sellers_1h > 0 else 0

    # Ratio unique wallets
    unique_wallet_ratio = buyers_1h / sellers_1h if sellers_1h > 0 else 1.0

    # === D√âTECTION 1: WHALE MANIPULATION (Achat/Vente) ===

    # FIX BUG #2: Seuils plus r√©alistes pour d√©tecter whale manipulation
    # avg_buys_per_buyer √©lev√© = concentration whale (m√™me avec beaucoup de buyers)

    # WHALE EXTREME: avg > 15 (whale tr√®s concentr√©e)
    if avg_buys_per_buyer > 15:
        signals.append("üêãüêã WHALE MANIPULATION EXTR√äME d√©tect√©e (avg: {:.1f}x buys/buyer)".format(avg_buys_per_buyer))
        whale_score -= 20  # GROS MALUS
        pattern = "WHALE_MANIPULATION"
        concentration_risk = "HIGH"

    # WHALE MOD√âR√âE: avg > 10 (concentration significative)
    elif avg_buys_per_buyer > 10:
        signals.append("üêã WHALE ACCUMULATION d√©tect√©e (avg: {:.1f}x buys/buyer)".format(avg_buys_per_buyer))
        whale_score -= 15  # MALUS car whale peut dumper
        pattern = "WHALE_MANIPULATION"
        concentration_risk = "HIGH"

    # WHALE SELL EXTREME: avg > 15 sells/seller
    elif avg_sells_per_seller > 15:
        signals.append("üö®üö® WHALE DUMP EXTR√äME d√©tect√© (avg: {:.1f}x sells/seller)".format(avg_sells_per_seller))
        whale_score -= 30  # √âNORME MALUS
        pattern = "WHALE_SELLING"
        concentration_risk = "HIGH"

    # WHALE SELL MOD√âR√âE: avg > 10 sells/seller
    elif avg_sells_per_seller > 10:
        signals.append("üö® WHALE DUMP d√©tect√© (avg: {:.1f}x sells/seller)".format(avg_sells_per_seller))
        whale_score -= 25  # GROS MALUS car dump imminent
        pattern = "WHALE_SELLING"
        concentration_risk = "HIGH"

    # WHALE SELL FAIBLE: avg > 5 sells/seller (seulement si peu de sellers)
    elif avg_sells_per_seller > 5 and sellers_1h < 50:
        signals.append("‚ö†Ô∏è WHALE SELLING d√©tect√©e (avg: {:.1f}x sells/seller)".format(avg_sells_per_seller))
        whale_score -= 15
        pattern = "WHALE_SELLING"
        concentration_risk = "MEDIUM"

    # === D√âTECTION 2: ACCUMULATION DISTRIBU√âE (BULLISH) ===

    # Beaucoup de buyers uniques + ratio favorable
    elif buyers_1h > sellers_1h * 1.5 and buyers_1h > 15:
        signals.append("‚úÖ ACCUMULATION DISTRIBU√âE (achat par many wallets)")
        whale_score += 15  # BONUS car accumulation saine
        pattern = "DISTRIBUTED_BUYING"
        concentration_risk = "LOW"

    # Buyers > sellers mais mod√©r√©
    elif buyers_1h > sellers_1h * 1.2 and buyers_1h > 10:
        signals.append("üìà Sentiment BULLISH (plus de buyers que sellers)")
        whale_score += 10
        pattern = "DISTRIBUTED_BUYING"
        concentration_risk = "LOW"

    # === D√âTECTION 3: DISTRIBUTION √âQUILIBR√âE ===

    elif 0.8 <= unique_wallet_ratio <= 1.2:
        signals.append("‚öñÔ∏è March√© √©quilibr√© (buyers ‚âà sellers)")
        whale_score += 0  # Neutre
        pattern = "NORMAL"
        concentration_risk = "MEDIUM"

    # === D√âTECTION 4: SELLING PRESSURE ===

    elif sellers_1h > buyers_1h * 1.3:
        signals.append("‚ö†Ô∏è SELLING PRESSURE (plus de sellers que buyers)")
        whale_score -= 10  # MALUS
        pattern = "DISTRIBUTED_SELLING"
        concentration_risk = "MEDIUM"

    else:
        pattern = "NORMAL"
        concentration_risk = "MEDIUM"

    # === V√âRIFICATION SUPPL√âMENTAIRE: Activit√© 24h ===

    # Si buyers_24h faible malgr√© volume √©lev√© ‚Üí concentration whale
    if buyers_24h > 0:
        avg_buys_per_buyer_24h = buys_24h / buyers_24h
        if avg_buys_per_buyer_24h > 8:
            signals.append("‚ö†Ô∏è Concentration whale sur 24h (peu de wallets uniques)")
            whale_score -= 8
            concentration_risk = "HIGH"

    return {
        'pattern': pattern,
        'whale_score': whale_score,
        'avg_buys_per_buyer': round(avg_buys_per_buyer, 2),
        'avg_sells_per_seller': round(avg_sells_per_seller, 2),
        'unique_wallet_ratio': round(unique_wallet_ratio, 2),
        'concentration_risk': concentration_risk,
        'signals': signals,
        'buyers_1h': buyers_1h,
        'sellers_1h': sellers_1h
    }

# ============================================
# V3: NOUVELLES FONCTIONS DE FILTRAGE
# Bas√© sur Backtest Phase 2 (3,261 alertes analys√©es)
# ============================================

def check_watchlist_token(pool_data: Dict) -> bool:
    """
    V√©rifie si le token est dans la watchlist "Money Printer"
    Ces tokens ont montr√© 77-100% win rate historique

    Returns:
        True si token dans watchlist
    """
    token_name = pool_data.get('token_name', '').lower()
    token_symbol = pool_data.get('token_symbol', '').lower()

    for watch_token in WATCHLIST_TOKENS:
        watch_lower = watch_token.lower()
        if watch_lower in token_name or watch_lower in token_symbol:
            return True

    return False

def filter_by_velocite(pool_data: Dict) -> Tuple[bool, str]:
    """
    Filtre V√âLOCIT√â - V3.1 avec seuils diff√©renci√©s par r√©seau

    Strat√©gie:
    - ETH/SOLANA: V√©locit√© min 10 (r√©seaux performants)
    - BASE: V√©locit√© min 15 (filtrage agressif, volume √©lev√©)
    - BSC: V√©locit√© min 12 (mod√©r√©)

    Returns:
        (pass_filter, reason)
    """
    velocite = pool_data.get('velocite_pump', 0)
    network = pool_data.get('network', '').lower()

    # Token watchlist: bypass filtre v√©locit√©
    if check_watchlist_token(pool_data):
        return True, "Watchlist token - bypass v√©locit√©"

    # V3.1: Seuil par r√©seau (si d√©fini), sinon seuil global
    min_velocity = NETWORK_SCORE_FILTERS.get(network, {}).get('min_velocity', MIN_VELOCITE_PUMP)

    # Filtre minimum CRITIQUE
    if velocite < min_velocity:
        return False, f"V√©locit√© trop faible: {velocite:.1f} < {min_velocity} ({network.upper()})"

    # Bonus si v√©locit√© optimale
    if velocite >= OPTIMAL_VELOCITE_PUMP:
        return True, f"V√©locit√© EXCELLENTE: {velocite:.1f} (>{OPTIMAL_VELOCITE_PUMP} = pattern gagnant)"

    return True, f"V√©locit√© OK: {velocite:.1f} (min {network.upper()}: {min_velocity})"

def filter_by_type_pump(pool_data: Dict) -> Tuple[bool, str]:
    """
    Filtre TYPE PUMP - 73% des losers sont "LENT"

    Returns:
        (pass_filter, reason)
    """
    type_pump = pool_data.get('type_pump', 'UNKNOWN')

    # Token watchlist: bypass filtre type
    if check_watchlist_token(pool_data):
        return True, "Watchlist token - bypass type pump"

    # Rejeter types probl√©matiques
    if type_pump in REJECTED_PUMP_TYPES:
        return False, f"Type pump rejet√©: {type_pump} (73% des √©checs)"

    # Accepter types performants
    if type_pump in ALLOWED_PUMP_TYPES:
        return True, f"Type pump OK: {type_pump}"

    # Type inconnu: accepter avec warning
    return True, f"Type pump inconnu: {type_pump} (√† v√©rifier)"

def filter_by_age(pool_data: Dict) -> Tuple[bool, str]:
    """
    Filtre √ÇGE TOKEN optimis√©
    - Zone optimale: 2-3 jours (36.1% WR, +234% ROI)
    - Zone danger: 12-24h (8.6% WR - PIRE moment)
    - Trop t√¥t (0-30min): 23.8% WR, -34% drawdown

    Returns:
        (pass_filter, reason)
    """
    age_hours = pool_data.get('age_hours', 0)

    # Token watchlist: bypass filtre √¢ge
    if check_watchlist_token(pool_data):
        return True, "Watchlist token - bypass √¢ge"

    # Trop vieux (7+ jours: 0% WR historique)
    if age_hours > MAX_TOKEN_AGE_HOURS:
        return False, f"Token trop vieux: {age_hours:.1f}h (>7 jours = 0% WR)"

    # Trop jeune (< 3h: risqu√©, -34% drawdown moyen)
    if age_hours < MIN_TOKEN_AGE_HOURS:
        # Exception: si v√©locit√© exceptionnelle (>50)
        velocite = pool_data.get('velocite_pump', 0)
        if velocite < OPTIMAL_VELOCITE_PUMP:
            return False, f"Token trop jeune: {age_hours:.1f}h (<3h risqu√©, drawdown -34%)"

    # ZONE DANGER 12-24h (8.6% WR - PIRE timing!)
    if DANGER_ZONE_AGE_MIN <= age_hours <= DANGER_ZONE_AGE_MAX:
        # Rejeter sauf si v√©locit√© exceptionnelle ET score √©lev√©
        velocite = pool_data.get('velocite_pump', 0)
        score = pool_data.get('score', 0)
        if velocite < OPTIMAL_VELOCITE_PUMP or score < 80:
            return False, f"ZONE DANGER √¢ge: {age_hours:.1f}h (12-24h = 8.6% WR!)"

    # ZONE OPTIMALE 2-3 jours (36.1% WR, +234% ROI)
    if OPTIMAL_TOKEN_AGE_MIN_HOURS <= age_hours <= OPTIMAL_TOKEN_AGE_MAX_HOURS:
        return True, f"√Çge OPTIMAL: {age_hours:.1f}h (2-3 jours = 36.1% WR!)"

    # V3.1: Zone embryonic 0-3h (Quality Index 182.83 - OPTIMAL!)
    if 0 <= age_hours <= EMBRYONIC_AGE_MAX_HOURS:
        velocite = pool_data.get('velocite_pump', 0)
        if velocite >= 20:  # Embryonic OK si v√©locit√© forte
            return True, f"√Çge EMBRYONIC OPTIMAL: {age_hours:.1f}h (QI 182.83!)"
        else:
            return True, f"√Çge embryonic: {age_hours:.1f}h (acceptable si v√©locit√© >20)"

    # Autres zones: acceptable
    return True, f"√Çge OK: {age_hours:.1f}h"

def filter_by_score_network(pool_data: Dict) -> Tuple[bool, str]:
    """
    Filtre SCORE par r√©seau - V3.1 NOUVEAU

    Strat√©gie diff√©renci√©e:
    - ETH (77.4% quality): Score min 85 (moins strict, r√©seau excellent)
    - BASE (59.2% quality): Score min 90 (strict, filtrer volume √©lev√©)
    - BSC (50.2% quality): Score min 88 (mod√©r√©)
    - SOLANA (39.2% quality): Score min 85 (moins strict, bon potentiel)

    Returns:
        (pass_filter, reason)
    """
    score = pool_data.get('score', 0)
    network = pool_data.get('network', '').lower()

    # Token watchlist: bypass filtre score
    if check_watchlist_token(pool_data):
        return True, "Watchlist token - bypass score"

    # V3.1: Seuil par r√©seau (si d√©fini), sinon global 85
    min_score = NETWORK_SCORE_FILTERS.get(network, {}).get('min_score', 85)

    if score < min_score:
        return False, f"Score insuffisant: {score} < {min_score} ({network.upper()})"

    return True, f"Score OK: {score} (min {network.upper()}: {min_score})"

def filter_by_liquidity_range(pool_data: Dict) -> Tuple[bool, str]:
    """
    Filtre LIQUIDIT√â par r√©seau (zones optimales)
    - Solana: $100K-$200K optimal (43.8% WR)
    - ETH: $100K-$200K optimal (55.6% WR, +6,987% ROI!)
    - BSC: $500K-$5M optimal (36-39% WR)

    Returns:
        (pass_filter, reason)
    """
    network = pool_data.get('network', 'unknown')
    liquidity = pool_data.get('liquidity', 0)

    # Token watchlist: bypass filtre liquidit√©
    if check_watchlist_token(pool_data):
        return True, "Watchlist token - bypass liquidit√©"

    thresholds = NETWORK_THRESHOLDS.get(network, NETWORK_THRESHOLDS.get("default"))

    # Check min
    min_liq = thresholds.get('min_liquidity', 0)
    if liquidity < min_liq:
        return False, f"Liquidit√© trop faible: ${liquidity:,.0f} < ${min_liq:,.0f}"

    # Check max (si d√©fini)
    max_liq = thresholds.get('max_liquidity')
    if max_liq and liquidity > max_liq:
        return False, f"Liquidit√© trop √©lev√©e: ${liquidity:,.0f} > ${max_liq:,.0f} (tokens gros = d√©j√† d√©couverts)"

    # Zone optimale par r√©seau
    if network == "solana" and 100000 <= liquidity <= 200000:
        return True, f"Liquidit√© OPTIMALE Solana: ${liquidity:,.0f} (43.8% WR!)"
    elif network == "eth" and 100000 <= liquidity <= 200000:
        return True, f"Liquidit√© JACKPOT ETH: ${liquidity:,.0f} (55.6% WR, +6,987% ROI!)"
    elif network == "bsc" and 500000 <= liquidity <= 5000000:
        return True, f"Liquidit√© OPTIMALE BSC: ${liquidity:,.0f} (36-39% WR)"

    return True, f"Liquidit√© OK: ${liquidity:,.0f}"

def apply_v3_filters(pool_data: Dict) -> Tuple[bool, List[str]]:
    """
    Applique TOUS les filtres V3 dans l'ordre d'importance
    Retourne (pass_all_filters, reasons_list)
    """
    reasons = []

    # 1. SCORE PAR R√âSEAU (V3.1 - NOUVEAU!)
    pass_score, reason_score = filter_by_score_network(pool_data)
    reasons.append(f"‚úì {reason_score}" if pass_score else f"‚úó {reason_score}")
    if not pass_score:
        return False, reasons

    # 2. V√âLOCIT√â PAR R√âSEAU (V3.1 - Am√©lior√© avec seuils diff√©renci√©s)
    pass_vel, reason_vel = filter_by_velocite(pool_data)
    reasons.append(f"‚úì {reason_vel}" if pass_vel else f"‚úó {reason_vel}")
    if not pass_vel:
        return False, reasons

    # 3. TYPE PUMP (73% des losers sont LENT)
    pass_type, reason_type = filter_by_type_pump(pool_data)
    reasons.append(f"‚úì {reason_type}" if pass_type else f"‚úó {reason_type}")
    if not pass_type:
        return False, reasons

    # 4. √ÇGE TOKEN (V3.1 - Hybride: embryonic 0-3h + mature 48-72h)
    pass_age, reason_age = filter_by_age(pool_data)
    reasons.append(f"‚úì {reason_age}" if pass_age else f"‚úó {reason_age}")
    if not pass_age:
        return False, reasons

    # 5. LIQUIDIT√â (zones optimales par r√©seau)
    pass_liq, reason_liq = filter_by_liquidity_range(pool_data)
    reasons.append(f"‚úì {reason_liq}" if pass_liq else f"‚úó {reason_liq}")
    if not pass_liq:
        return False, reasons

    # Tous les filtres V3.1 pass√©s!
    return True, reasons

def calculate_confidence_tier(pool_data: Dict) -> str:
    """
    Calcule le tier de confiance: HIGH, MEDIUM, ou LOW
    Bas√© sur patterns gagnants du backtest

    HIGH = 35-50% WR attendu
    MEDIUM = 25-30% WR attendu
    LOW = 15-20% WR attendu
    """
    score = pool_data.get('score', 0)
    velocite = pool_data.get('velocite_pump', 0)
    type_pump = pool_data.get('type_pump', '')
    network = pool_data.get('network', '')
    age_hours = pool_data.get('age_hours', 0)
    liquidity = pool_data.get('liquidity', 0)

    # Token watchlist = AUTO HIGH
    if check_watchlist_token(pool_data):
        return "ULTRA_HIGH"  # 77-100% WR historique!

    # HIGH CONFIDENCE (attendu: 35-50% WR)
    high_conditions = [
        network in ['eth', 'solana'],
        48 <= age_hours <= 72,  # 2-3 jours
        velocite >= OPTIMAL_VELOCITE_PUMP,  # >50
        type_pump in ['PARABOLIQUE', 'TRES_RAPIDE'],
        100000 <= liquidity <= 300000
    ]

    if sum(high_conditions) >= 4:  # 4 sur 5 crit√®res
        return "HIGH"

    # MEDIUM CONFIDENCE (attendu: 25-30% WR)
    medium_conditions = [
        network in ['eth', 'solana', 'bsc'],
        age_hours >= 6,
        velocite >= 10,
        type_pump in ALLOWED_PUMP_TYPES,
        liquidity >= 50000,
        score >= 60
    ]

    if sum(medium_conditions) >= 4:
        return "MEDIUM"

    # LOW CONFIDENCE (attendu: 15-20% WR)
    if velocite >= MIN_VELOCITE_PUMP and type_pump in ALLOWED_PUMP_TYPES:
        return "LOW"

    # Tr√®s faible confiance
    return "VERY_LOW"

def calculate_final_score(pool_data: Dict, momentum: Dict, multi_pool_data: Dict) -> Tuple[int, int, int, Dict]:
    """
    Score final = base + momentum + whale_score.

    Returns:
        (final_score, base_score, momentum_bonus, whale_analysis)
    """
    base = calculate_base_score(pool_data)
    momentum_bonus = calculate_momentum_bonus(pool_data, momentum, multi_pool_data)

    # NOUVEAU: Analyse whale activity
    whale_analysis = analyze_whale_activity(pool_data)
    whale_score = whale_analysis['whale_score']

    # Score final avec whale bonus/malus
    final = base + momentum_bonus + whale_score
    final = max(min(final, 100), 0)  # Entre 0 et 100

    return final, base, momentum_bonus, whale_analysis

def calculate_confidence_score(pool_data: Dict) -> int:
    """
    Calcule niveau de confiance dans les donn√©es (0-100%).
    Corrig√© pour ALMANAK: ajout indicateur fiabilit√©.
    """
    confidence = 100

    # R√©duire confiance si donn√©es incompl√®tes
    if pool_data.get("network", "").lower() == "unknown":
        confidence -= 20  # Donn√©es r√©seau manquantes

    # R√©duire confiance si liquidit√© faible = donn√©es moins stables
    liq = pool_data.get("liquidity", 0)
    if liq < 200000:
        confidence -= 20  # Liquidit√© trop faible
    elif liq < 300000:
        confidence -= 10  # Liquidit√© moyenne-faible

    # R√©duire confiance si trop r√©cent = historique limit√©
    age = pool_data.get("age_hours", 999)
    if age < 6:
        confidence -= 20  # Moins de 6h = tr√®s peu d'historique
    elif age < 12:
        confidence -= 10  # Moins de 12h = historique limit√©

    # R√©duire confiance si volume tr√®s faible
    vol = pool_data.get("volume_24h", 0)
    if vol < 100000:
        confidence -= 15  # Volume faible = donn√©es peu significatives

    # R√©duire confiance si transactions trop peu nombreuses
    txns = pool_data.get("total_txns", 0)
    if txns < 200:
        confidence -= 10  # Peu de transactions = donn√©es peu repr√©sentatives

    return max(confidence, 0)

# ============================================
# D√âTECTION SIGNAUX
# ============================================
def detect_signals(pool_data: Dict, momentum: Dict, multi_pool_data: Dict) -> List[str]:
    """
    D√©tecte tous les signaux importants.
    Corrig√© pour ALMANAK: ajoute warnings pour dead cat bounce et panic sell.
    """
    # VALIDATION: V√©rifier que momentum est un dict valide
    if not momentum or not isinstance(momentum, dict):
        log(f"   ‚ö†Ô∏è momentum invalide dans detect_signals: {type(momentum)}")
        return []  # Pas de signaux si momentum invalide

    signals = []

    price_1h = momentum.get("1h", 0)
    price_6h = pool_data.get("price_change_6h", 0)
    price_24h = pool_data.get("price_change_24h", 0)

    buy_ratio_1h = pool_data["buys_1h"] / pool_data["sells_1h"] if pool_data["sells_1h"] > 0 else 1.0
    buy_ratio_24h = pool_data["buys_24h"] / pool_data["sells_24h"] if pool_data["sells_24h"] > 0 else 1.0

    # === DEAD CAT BOUNCE WARNING (NOUVEAU - Correction ALMANAK) ===
    if price_1h > 0 and price_24h < -20:
        if price_1h < 10:
            signals.append(f"‚ö†Ô∏è DEAD CAT BOUNCE: Rebond +{price_1h:.1f}% sur tendance baissi√®re {price_24h:.1f}%")
        else:
            signals.append(f"‚ö†Ô∏è REVERSAL FRAGILE: Rebond fort (+{price_1h:.1f}%) mais tendance 24h n√©gative ({price_24h:.1f}%)")

    # ACCELERATION (seulement si pas dead cat bounce)
    elif price_1h >= 5:
        signals.append(f"üöÄ ACCELERATION: +{price_1h:.1f}% en 1h")

    # REVERSAL (uniquement si contexte positif)
    if price_24h < -10 and price_1h > 3 and price_6h > 0:
        # Reversal confirm√© si 6h aussi positif
        signals.append(f"üìà REVERSAL CONFIRM√â: Hausse 1h/6h malgr√© {price_24h:.1f}% 24h")
    elif price_24h < -10 and price_1h > 5:
        # Reversal possible mais non confirm√©
        signals.append(f"üìà REVERSAL: Hausse malgr√© {price_24h:.1f}% 24h (confirmation n√©cessaire)")

    # === VOLUME SPIKE: QUALIFI√â (NOUVEAU) ===
    vol_1h_normalized = pool_data["volume_1h"] * 24
    if vol_1h_normalized > pool_data["volume_24h"] * 1.5:
        spike_pct = ((vol_1h_normalized / pool_data["volume_24h"]) - 1) * 100

        # Qualifier le spike
        if price_1h > 3:
            signals.append(f"üìä VOLUME SPIKE BULLISH: +{spike_pct:.0f}% activit√© + prix hausse")
        elif price_1h < -5:
            signals.append(f"üö® VOLUME SPIKE BEARISH: +{spike_pct:.0f}% activit√© = panic sell d√©tect√©")
        else:
            signals.append(f"üìä VOLUME SPIKE: +{spike_pct:.0f}% activit√© vs moyenne")

    # === BUY PRESSURE (AM√âLIOR√â) ===
    if buy_ratio_1h > buy_ratio_24h * 1.2 and buy_ratio_1h >= 0.8:
        signals.append(f"üü¢ BUY PRESSURE: Ratio 1h ({buy_ratio_1h:.2f}) > 24h ({buy_ratio_24h:.2f})")
    elif buy_ratio_1h < buy_ratio_24h * 0.8 and buy_ratio_24h < 0.5:
        # SELL PRESSURE warning (NOUVEAU)
        signals.append(f"üî¥ SELL PRESSURE: Vendeurs intensifient (1h: {buy_ratio_1h:.2f}, 24h: {buy_ratio_24h:.2f})")

    # MULTI-POOL
    if multi_pool_data.get("is_multi_pool"):
        signals.append(f"üåê MULTI-POOL: {multi_pool_data['num_pools']} pools actifs")
        if multi_pool_data.get("is_weth_dominant"):
            signals.append(f"‚ö° WETH pool dominant = Smart money")

    # === BOTTOM CONFIRM√â (AM√âLIOR√â - plus strict) ===
    # Bottom = prix bas + acheteurs reviennent + pression change
    if price_24h < -15 and price_1h > 0 and buy_ratio_1h > buy_ratio_24h * 1.3 and buy_ratio_1h > 0.9:
        signals.append(f"‚úÖ BOTTOM CONFIRM√â: Acheteurs reviennent massivement")
    elif price_24h < -10 and price_1h > 3 and price_6h > 0:
        signals.append(f"‚úÖ Bottom potentiel: Reversal multi-timeframe")

    return signals

# ============================================
# VALIDATION OPPORTUNIT√â
# ============================================
def is_valid_opportunity(pool_data: Dict, score: int) -> Tuple[bool, str]:
    """
    V√©rifie si pool est une opportunit√© valide.
    V3: Applique TOUS les filtres backtest avant validation classique.
    """

    # ===== V3: APPLIQUER FILTRES BACKTEST EN PRIORIT√â =====
    passes_v3, v3_reasons = apply_v3_filters(pool_data)

    # Stocker les raisons V3 dans pool_data pour affichage ult√©rieur
    pool_data['v3_filter_reasons'] = v3_reasons

    # Si √©chec filtres V3, rejeter imm√©diatement (sauf watchlist)
    if not passes_v3:
        # Concat√©ner toutes les raisons d'√©chec
        failed_reasons = [r for r in v3_reasons if r.startswith('‚úó')]
        if failed_reasons:
            return False, f"[V3 REJECT] {failed_reasons[0].replace('‚úó ', '')}"
        return False, "[V3 REJECT] Filtres backtest non satisfaits"

    # ===== VALIDATION CLASSIQUE (si V3 pass√©) =====

    # R√©cup√©rer seuils par r√©seau (avec fallback sur default)
    network = pool_data.get("network", "")
    thresholds = NETWORK_THRESHOLDS.get(network, NETWORK_THRESHOLDS["default"])

    min_liq = thresholds["min_liquidity"]
    min_vol = thresholds["min_volume"]
    min_txns = thresholds["min_txns"]

    # Check liquidit√© min (d√©j√† v√©rifi√© par V3 mais double s√©curit√©)
    if pool_data["liquidity"] < min_liq:
        return False, f"‚ùå Liquidit√© trop faible: ${pool_data['liquidity']:,.0f}"

    # Check volume min
    if pool_data["volume_24h"] < min_vol:
        return False, f"‚ö†Ô∏è Volume trop faible: ${pool_data['volume_24h']:,.0f}"

    # Check transactions min
    if pool_data["total_txns"] < min_txns:
        return False, f"‚ö†Ô∏è Pas assez de txns: {pool_data['total_txns']}"

    # Check age max (d√©j√† v√©rifi√© par V3)
    if pool_data["age_hours"] > MAX_TOKEN_AGE_HOURS:
        return False, f"‚è≥ Token trop ancien: {pool_data['age_hours']:.0f}h"

    # Check score minimum (ASSOUPLI pour backtesting: 55 ‚Üí 50)
    if score < 50:
        return False, f"üìâ Score trop faible: {score}/100"

    # Check ratio volume/liquidit√©
    ratio = pool_data["volume_24h"] / pool_data["liquidity"] if pool_data["liquidity"] > 0 else 0
    if ratio < VOLUME_LIQUIDITY_RATIO:
        return False, f"üìâ Ratio Vol/Liq trop faible: {ratio:.1%}"

    # Check pump & dump potentiel
    buy_sell_ratio = pool_data["buys_24h"] / pool_data["sells_24h"] if pool_data["sells_24h"] > 0 else 999
    if buy_sell_ratio > 5:
        return False, f"üö® Trop d'achats vs ventes (pump?): {buy_sell_ratio:.1f}"
    if buy_sell_ratio < 0.2:
        return False, f"üìâ Trop de ventes vs achats (dump?): {buy_sell_ratio:.1f}"

    return True, "‚úÖ Opportunit√© valide [V3 APPROVED]"

# ============================================
# √âVALUATION MARCH√â POUR D√âCISION D'ENTR√âE
# ============================================
def evaluer_conditions_marche(pool_data: Dict, score: int, momentum: Dict,
                              signal_1h: str = None, signal_6h: str = None) -> tuple:
    """
    √âvalue TOUTES les conditions du march√© pour d√©cider si afficher ACTION RECOMMAND√âE.

    Returns:
        (bool, str, list): (should_enter, decision_type, reasons)
        - should_enter: True = afficher Entry/SL/TP, False = afficher analyse de sortie
        - decision_type: "BUY", "WAIT", "EXIT"
        - reasons: Liste des raisons qui justifient la d√©cision
    """

    reasons_bullish = []
    reasons_bearish = []
    reasons_neutral = []

    # Extraire les donn√©es
    pct_24h = pool_data.get("price_change_24h", 0)
    pct_6h = pool_data.get("price_change_6h", 0)
    pct_1h = pool_data.get("price_change_1h", 0)
    vol_24h = pool_data.get("volume_24h", 0)
    vol_6h = pool_data.get("volume_6h", 0)
    vol_1h = pool_data.get("volume_1h", 0)
    liq = pool_data.get("liquidity", 0)
    buys = pool_data.get("buys_24h", 0)
    sells = pool_data.get("sells_24h", 0)
    buys_1h = pool_data.get("buys_1h", 0)
    sells_1h = pool_data.get("sells_1h", 0)
    age = pool_data.get("age_hours", 0)

    buy_ratio_24h = buys / sells if sells > 0 else 1.0
    buy_ratio_1h = buys_1h / sells_1h if sells_1h > 0 else 1.0

    # ===== 1. ANALYSE DU SCORE =====
    if score >= 80:
        reasons_bullish.append("Score excellent (‚â•80)")
    elif score >= 70:
        reasons_bullish.append("Score bon (‚â•70)")
    elif score < 60:
        reasons_bearish.append(f"Score faible ({score})")

    # ===== 2. ANALYSE VOLUME (CRITIQUE) =====
    if vol_24h > 0 and vol_6h > 0 and vol_1h > 0:
        vol_24h_avg = vol_24h / 24
        vol_6h_avg = vol_6h / 6
        ratio_1h_vs_6h = (vol_1h / vol_6h_avg) if vol_6h_avg > 0 else 0
        ratio_6h_vs_24h = (vol_6h_avg / vol_24h_avg) if vol_24h_avg > 0 else 0

        # Volume court terme (1h vs 6h)
        if signal_1h == "FORTE_ACCELERATION":
            reasons_bullish.append("Volume 1h en FORTE acc√©l√©ration")
        elif signal_1h == "ACCELERATION":
            reasons_bullish.append("Volume 1h en acc√©l√©ration")
        elif signal_1h == "RALENTISSEMENT":
            reasons_bearish.append("Volume 1h en RALENTISSEMENT")
        elif signal_1h == "FORT_RALENTISSEMENT":
            reasons_bearish.append("Volume 1h en FORT RALENTISSEMENT")

        # Volume moyen terme (6h vs 24h)
        if signal_6h == "PUMP_EN_COURS":
            reasons_bullish.append("Pump confirm√© sur 6h")
        elif signal_6h == "HAUSSE_PROGRESSIVE":
            reasons_bullish.append("Hausse progressive")
        elif signal_6h == "BAISSE_TENDANCIELLE":
            reasons_bearish.append("Baisse tendancielle sur 6h")

        # PATTERNS CRITIQUES
        if signal_1h in ["FORTE_ACCELERATION", "ACCELERATION"] and signal_6h == "PUMP_EN_COURS":
            reasons_bullish.append("üéØ PATTERN ID√âAL: Pump actif + acc√©l√©ration")
        elif signal_1h in ["RALENTISSEMENT", "FORT_RALENTISSEMENT"] and signal_6h == "PUMP_EN_COURS":
            reasons_bearish.append("‚ö†Ô∏è PATTERN SORTIE: Essoufflement d√©tect√©")
        elif signal_1h in ["RALENTISSEMENT", "FORT_RALENTISSEMENT"]:
            reasons_bearish.append("üî¥ PATTERN √âVITER: Volume en chute")

    # ===== 3. ANALYSE PRIX / MOMENTUM (FIX BUG #4 - Multi-TF Confluence) =====

    # Tendance prix 24h
    if pct_24h >= 20:
        reasons_bullish.append(f"Prix 24h en hausse forte (+{pct_24h:.1f}%)")
    elif pct_24h >= 5:
        reasons_bullish.append(f"Prix 24h en hausse (+{pct_24h:.1f}%)")
    elif pct_24h < -15:
        reasons_bearish.append(f"Prix 24h en baisse ({pct_24h:.1f}%)")

    # NOUVEAU: Multi-Timeframe Confluence (Quick Win #3)
    # D√©tecter PULLBACK SAIN sur uptrend (buy the dip)
    if pct_24h >= 5 and -8 < pct_1h < 0:
        # Uptrend 24h + pullback l√©ger 1h = BUY THE DIP
        reasons_bullish.append(f"üìä PULLBACK SAIN: +{pct_24h:.1f}% 24h | {pct_1h:.1f}% 1h (buy the dip)")
        reasons_bullish.append("‚úÖ Multi-TF confluence: Opportunit√© d'entr√©e sur retracement")
    # D√©tecter continuation haussi√®re (multi-TF bullish)
    elif pct_24h >= 5 and pct_6h >= 3 and pct_1h >= 2:
        reasons_bullish.append(f"üöÄ MULTI-TF BULLISH: Hausse confirm√©e sur 24h/6h/1h")
    # Tendance prix court terme (si pas de pullback sain)
    elif pct_1h >= 5:
        reasons_bullish.append(f"Momentum 1h positif (+{pct_1h:.1f}%)")
    elif pct_1h <= -10:
        # Seulement consid√©rer bearish si vraiment n√©gatif (-10%)
        reasons_bearish.append(f"Momentum 1h tr√®s n√©gatif ({pct_1h:.1f}%)")

    # Analyse de la d√©c√©l√©ration (CRITIQUE pour sortie)
    # MODIFI√â: Seulement si AUCUN pullback sain
    if pct_1h > 0 and pct_6h > 0 and not (pct_24h >= 5 and -8 < pct_1h < 0):
        if pct_1h < pct_6h * 0.5:  # 1h fait moins de 50% du 6h = d√©c√©l√©ration
            reasons_bearish.append("D√©c√©l√©ration: momentum 1h < 50% du 6h")

    # ===== 4. ANALYSE PRESSION ACHAT/VENTE =====
    ratio_change = buy_ratio_1h - buy_ratio_24h

    if ratio_change > 0.15:  # Forte augmentation pression acheteuse
        reasons_bullish.append(f"Pression acheteuse en hausse (+{ratio_change:.1%})")
    elif ratio_change < -0.15:  # Forte augmentation pression vendeuse
        reasons_bearish.append(f"Pression vendeuse en hausse ({ratio_change:.1%})")

    if buy_ratio_1h >= 1.3:
        reasons_bullish.append(f"Acheteurs dominent 1h (ratio {buy_ratio_1h:.2f})")
    elif buy_ratio_1h <= 0.7:
        reasons_bearish.append(f"Vendeurs dominent 1h (ratio {buy_ratio_1h:.2f})")

    # ===== 5. ANALYSE LIQUIDIT√â =====
    if liq < 150000:
        reasons_bearish.append(f"Liquidit√© tr√®s faible (${liq/1000:.0f}K) - Risque rug √©lev√©")
    elif liq < 200000:
        reasons_neutral.append(f"Liquidit√© faible (${liq/1000:.0f}K) - Prudence")
    elif liq >= 500000:
        reasons_bullish.append(f"Liquidit√© solide (${liq/1000:.0f}K)")

    # ===== 6. ANALYSE √ÇGE =====
    if age > 48:
        reasons_neutral.append(f"Token mature ({age:.0f}h) - Exit window peut √™tre pass√©e")
    elif age < 1:
        reasons_neutral.append(f"Token tr√®s jeune ({age:.1f}h) - Volatilit√© extr√™me")

    # ===== D√âCISION FINALE (FIX BUG #6 - Score 70+ devrait donner BUY) =====
    score_bullish = len(reasons_bullish)
    score_bearish = len(reasons_bearish)

    # D√©tecter patterns critiques
    has_critical_bullish = any("PATTERN ID√âAL" in r or "FORTE acc√©l√©ration" in r or "MULTI-TF BULLISH" in r or "PULLBACK SAIN" in r for r in reasons_bullish)
    has_critical_bearish = any("PATTERN SORTIE" in r or "PATTERN √âVITER" in r or "FORT RALENTISSEMENT" in r for r in reasons_bearish)

    # NOUVELLE LOGIQUE:
    # 1. Score 70+ avec multi-TF confluence ‚Üí BUY
    # 2. Pattern critique bearish ‚Üí EXIT
    # 3. Pattern critique bullish + score >= 65 ‚Üí BUY
    # 4. Score bullish dominant ‚Üí BUY
    # 5. Sinon ‚Üí WAIT ou EXIT

    if has_critical_bearish:
        # Bearish critique = SORTIR (m√™me si score √©lev√©)
        decision = "EXIT"
        should_enter = False
    elif score >= 75 and score_bullish >= 3 and score_bearish <= 1:
        # Score excellent + plusieurs signaux bullish = BUY
        decision = "BUY"
        should_enter = True
    elif score >= 70 and (has_critical_bullish or score_bullish >= 2) and score_bearish <= 1:
        # Score bon + signaux bullish = BUY
        decision = "BUY"
        should_enter = True
    elif has_critical_bullish and score >= 65 and score_bearish <= 2:
        # Pattern id√©al/Multi-TF/Pullback sain + score OK = BUY
        decision = "BUY"
        should_enter = True
    elif score_bullish >= 4 and score_bearish <= 1:
        # Beaucoup de signaux bullish = BUY
        decision = "BUY"
        should_enter = True
    elif score_bearish >= 3 or score < 60:
        # Trop bearish ou score faible = EXIT
        decision = "EXIT"
        should_enter = False
    elif score_bullish >= 2 and score_bearish <= 2:
        # Mitig√© = WAIT
        decision = "WAIT"
        should_enter = False
    else:
        # D√©faut = EXIT
        decision = "EXIT"
        should_enter = False

    return should_enter, decision, {
        'bullish': reasons_bullish,
        'bearish': reasons_bearish,
        'neutral': reasons_neutral
    }

# ============================================
# ANALYSE ALERTE SUIVANTE (TP TRACKING)
# ============================================
def analyser_alerte_suivante(previous_alert: Dict, current_price: float, pool_data: Dict,
                             score: int, momentum: Dict, signal_1h: str = None,
                             signal_6h: str = None, tracker=None) -> Dict:
    """
    Analyse une alerte suivante sur un token d√©j√† alert√©.
    V√©rifie si les TP ont √©t√© atteints et d√©cide de la strat√©gie.

    VERSION SIMPLE+ - 5 R√àGLES ESSENTIELLES:
    1. D√©tection des TP atteints
    2. V√©rification du prix (pas trop √©lev√© pour re-entry)
    3. R√©√©valuation des conditions actuelles
    4. D√©cision: NOUVEAUX_NIVEAUX / SECURISER_HOLD / SORTIR
    5. Analyse v√©locit√© du pump (protection pump parabolique)

    Args:
        previous_alert: Derni√®re alerte sur ce token (depuis DB)
        current_price: Prix actuel du token
        pool_data: Donn√©es actuelles du pool
        score: Score actuel
        momentum: Momentum actuel
        signal_1h: Signal volume 1h vs 6h
        signal_6h: Signal volume 6h vs 24h

    Returns:
        Dict avec:
            - decision: "NOUVEAUX_NIVEAUX" / "SECURISER_HOLD" / "SORTIR"
            - tp_hit: Liste des TP atteints ["TP1", "TP2", "TP3"]
            - tp_gains: Dict avec les gains r√©alis√©s {"TP1": 5.0, ...}
            - prix_trop_eleve: bool
            - conditions_favorables: bool
            - raisons: Liste des raisons de la d√©cision
            - nouveaux_niveaux: Dict (si applicable) avec entry/sl/tp
            - velocite_pump: float (% par heure)
            - type_pump: str (PARABOLIQUE / RAPIDE / NORMAL / LENT)
    """

    # VALIDATION: V√©rifier que previous_alert et pool_data sont valides
    if not previous_alert or not isinstance(previous_alert, dict):
        log(f"   ‚ö†Ô∏è previous_alert invalide: {type(previous_alert)}")
        return {
            'decision': 'ERROR',
            'tp_hit': [],
            'tp_gains': {},
            'prix_trop_eleve': False,
            'conditions_favorables': False,
            'raisons': ["Donn√©es d'alerte pr√©c√©dente invalides"],
            'nouveaux_niveaux': {},
            'hausse_depuis_alerte': 0,
            'velocite_pump': 0,
            'type_pump': 'UNKNOWN',
            'temps_ecoule_heures': 0
        }

    if not pool_data or not isinstance(pool_data, dict):
        log(f"   ‚ö†Ô∏è pool_data invalide dans analyser_alerte_suivante: {type(pool_data)}")
        return {
            'decision': 'ERROR',
            'tp_hit': [],
            'tp_gains': {},
            'prix_trop_eleve': False,
            'conditions_favorables': False,
            'raisons': ["Donn√©es de pool invalides"],
            'nouveaux_niveaux': {},
            'hausse_depuis_alerte': 0,
            'velocite_pump': 0,
            'type_pump': 'UNKNOWN',
            'temps_ecoule_heures': 0
        }

    if not momentum or not isinstance(momentum, dict):
        log(f"   ‚ö†Ô∏è momentum invalide dans analyser_alerte_suivante: {type(momentum)}")
        return {
            'decision': 'ERROR',
            'tp_hit': [],
            'tp_gains': {},
            'prix_trop_eleve': False,
            'conditions_favorables': False,
            'raisons': ["Donn√©es de momentum invalides"],
            'nouveaux_niveaux': {},
            'hausse_depuis_alerte': 0,
            'velocite_pump': 0,
            'type_pump': 'UNKNOWN',
            'temps_ecoule_heures': 0
        }

    # R√àGLE 1: D√©tection des TP atteints
    # IMPORTANT: On v√©rifie si les TP ont √©t√© atteints DANS LE PASS√â (pas juste le prix actuel)
    tp_hit = []
    tp_gains = {}

    tp1_price = previous_alert.get('tp1_price', 0)
    tp2_price = previous_alert.get('tp2_price', 0)
    tp3_price = previous_alert.get('tp3_price', 0)
    entry_price = previous_alert.get('entry_price', previous_alert.get('price_at_alert', 0))

    # R√©cup√©rer le prix MAX atteint depuis l'alerte pr√©c√©dente (depuis price_tracking)
    # Si pas de tracking disponible, utiliser le prix actuel comme fallback
    alert_id = previous_alert.get('id', 0)
    prix_max_atteint = current_price  # Fallback par d√©faut

    # Si le tracker est disponible, r√©cup√©rer le VRAI prix MAX depuis la DB
    if tracker is not None and alert_id > 0:
        prix_max_db = tracker.get_highest_price_for_alert(alert_id)
        if prix_max_db is not None:
            # Comparer avec le prix actuel et prendre le max
            prix_max_atteint = max(prix_max_db, current_price)
            # Note: On prend le max car le prix actuel peut √™tre > que le dernier tracking

    # FIX HARMONISATION: Tol√©rance 0.5% pour √©viter probl√®mes d'arrondi
    # Exemple: TP1=$0.1575, prix=$0.1574 ‚Üí consid√©r√© comme atteint (√©cart 0.06%)
    TP_TOLERANCE_PERCENT = 0.5  # 0.5% de tol√©rance

    def tp_reached(prix: float, tp_target: float) -> bool:
        """V√©rifie si TP atteint avec tol√©rance pour arrondi."""
        if tp_target <= 0:
            return False
        ecart_percent = ((prix - tp_target) / tp_target) * 100
        # TP atteint si prix >= TP - 0.5%
        return ecart_percent >= -TP_TOLERANCE_PERCENT

    # DEBUG: Log pour comprendre d√©tection TP
    if alert_id > 0:
        log(f"   üîç DEBUG TP: prix_max={prix_max_atteint:.8f}, tp1={tp1_price:.8f}, tp2={tp2_price:.8f}, tp3={tp3_price:.8f}")

    # V√©rification des TP bas√©e sur le prix MAX atteint (historique + actuel)
    # AVEC TOL√âRANCE pour √©viter probl√®mes d'arrondi
    if tp_reached(prix_max_atteint, tp3_price):
        tp_hit.extend(["TP1", "TP2", "TP3"])
        tp_gains["TP1"] = ((tp1_price - entry_price) / entry_price) * 100
        tp_gains["TP2"] = ((tp2_price - entry_price) / entry_price) * 100
        tp_gains["TP3"] = ((tp3_price - entry_price) / entry_price) * 100
    elif tp_reached(prix_max_atteint, tp2_price):
        tp_hit.extend(["TP1", "TP2"])
        tp_gains["TP1"] = ((tp1_price - entry_price) / entry_price) * 100
        tp_gains["TP2"] = ((tp2_price - entry_price) / entry_price) * 100
    elif tp_reached(prix_max_atteint, tp1_price):
        tp_hit.append("TP1")
        tp_gains["TP1"] = ((tp1_price - entry_price) / entry_price) * 100

    # R√àGLE 2: V√©rifier si le prix est trop √©lev√© pour re-entry (>20% depuis alerte initiale)
    hausse_depuis_alerte = ((current_price - entry_price) / entry_price) * 100
    prix_trop_eleve = hausse_depuis_alerte > 20.0

    # R√àGLE 5: Analyser la v√©locit√© du pump (protection pump parabolique)
    from datetime import datetime

    # Calculer le temps √©coul√© depuis l'alerte pr√©c√©dente
    try:
        if isinstance(previous_alert.get('created_at'), str):
            created_at = datetime.strptime(previous_alert['created_at'], '%Y-%m-%d %H:%M:%S')
        else:
            created_at = previous_alert.get('created_at')

        temps_ecoule_heures = (datetime.now() - created_at).total_seconds() / 3600
    except:
        # Si erreur de parsing, estimer √† 1h par d√©faut
        temps_ecoule_heures = 1.0

    # √âviter division par z√©ro
    if temps_ecoule_heures < 0.01:  # Moins de 36 secondes
        temps_ecoule_heures = 0.01

    # Calculer la v√©locit√©: % de hausse par heure
    velocite_pump = hausse_depuis_alerte / temps_ecoule_heures

    # Classifier le type de pump
    pump_parabolique = False
    pump_tres_rapide = False
    type_pump = ""

    if velocite_pump > 100:  # >100% par heure = PARABOLIQUE
        type_pump = "PARABOLIQUE"
        pump_parabolique = True
    elif velocite_pump > 50:  # >50% par heure = TR√àS RAPIDE
        type_pump = "TRES_RAPIDE"
        pump_tres_rapide = True
    elif velocite_pump > 20:  # >20% par heure = RAPIDE
        type_pump = "RAPIDE"
    elif velocite_pump > 5:  # >5% par heure = NORMAL
        type_pump = "NORMAL"
    else:  # ‚â§5% par heure = LENT (sain)
        type_pump = "LENT"

    # R√àGLE 3: R√©√©valuer les conditions actuelles du march√©
    log(f"   üîç DEBUG avant evaluer_conditions_marche: pool_data={type(pool_data)}, score={score}, momentum={type(momentum)}, signal_1h={signal_1h}, signal_6h={signal_6h}")
    conditions_favorables, decision_marche, raisons_marche = evaluer_conditions_marche(
        pool_data, score, momentum, signal_1h, signal_6h
    )
    log(f"   üîç DEBUG apr√®s evaluer_conditions_marche: raisons_marche={type(raisons_marche)}")

    # R√àGLE 4: D√©cision finale
    raisons = []
    decision = ""
    nouveaux_niveaux = {}

    # CAS 1: Aucun TP atteint ‚Üí √âvaluation selon conditions (FIX BUG #3)
    if not tp_hit:
        # √âvaluer si c'est toujours une bonne opportunit√© d'entr√©e
        if conditions_favorables and score >= 70:
            decision = "ENTRER"
            raisons.append(f"Aucun TP atteint mais conditions excellentes (Score: {score})")
            raisons.append(f"üí° Si pas en position: ENTRER maintenant")
            raisons.append(f"üí° Si d√©j√† en position: MAINTENIR (pas de TP atteint)")
            log(f"   üîç DEBUG AVANT extend bullish: raisons_marche type={type(raisons_marche)}, bullish type={type(raisons_marche.get('bullish') if isinstance(raisons_marche, dict) else 'N/A')}")
            raisons.extend(raisons_marche['bullish'][:3])
            log(f"   üîç DEBUG APR√àS extend bullish")
        elif conditions_favorables and score >= 60:
            decision = "ATTENDRE"
            raisons.append(f"Aucun TP atteint, conditions moyennes (Score: {score})")
            raisons.append(f"üí° Si pas en position: ATTENDRE meilleure entr√©e")
            raisons.append(f"üí° Si d√©j√† en position: MAINTENIR position initiale")
        else:
            decision = "EVITER"
            raisons.append("Aucun TP atteint et conditions d√©favorables")
            raisons.append(f"üí° Si pas en position: √âVITER")
            raisons.append(f"üí° Si en position: Consid√©rer SORTIE si SL proche")
            if raisons_marche['bearish']:
                raisons.extend(raisons_marche['bearish'][:2])

    # CAS 2a: PUMP PARABOLIQUE ‚Üí SORTIR IMM√âDIATEMENT (risque dump violent)
    elif pump_parabolique and tp_hit:
        decision = "SORTIR"
        raisons.append(f"‚úÖ {', '.join(tp_hit)} atteint(s) (+{hausse_depuis_alerte:.1f}%)")
        raisons.append(f"üö® PUMP PARABOLIQUE d√©tect√© ({velocite_pump:.0f}%/h)")
        raisons.append(f"‚ö†Ô∏è Risque de dump violent - S√âCURISER IMM√âDIATEMENT")
        raisons.append("üí∞ Prendre les profits maintenant avant le retournement")

    # CAS 2b: TP atteint(s) + prix trop √©lev√© ‚Üí Ne pas re-rentrer
    elif prix_trop_eleve:
        decision = "SORTIR"
        raisons.append(f"‚úÖ {', '.join(tp_hit)} atteint(s) (+{hausse_depuis_alerte:.1f}%)")
        raisons.append(f"‚ö†Ô∏è Prix trop √©lev√© pour re-entry (+{hausse_depuis_alerte:.1f}% depuis alerte initiale)")
        raisons.append("üí∞ S√©curiser les gains d√©j√† r√©alis√©s")

    # CAS 3a: PUMP TR√àS RAPIDE + conditions favorables ‚Üí Nouveaux niveaux TR√àS SERR√âS
    elif pump_tres_rapide and conditions_favorables and tp_hit:
        decision = "NOUVEAUX_NIVEAUX"
        raisons.append(f"‚úÖ {', '.join(tp_hit)} atteint(s)")
        for tp_name, gain in tp_gains.items():
            raisons.append(f"   {tp_name}: +{gain:.1f}%")
        raisons.append(f"‚ö° Pump tr√®s rapide ({velocite_pump:.0f}%/h)")
        raisons.append(f"üöÄ Conditions encore favorables ({decision_marche})")
        raisons.append("‚ö†Ô∏è SL TR√àS SERR√â (-3%) car pump rapide")

        # SL TR√àS SERR√â √† 97% pour pump tr√®s rapide
        nouveaux_niveaux = {
            'entry_price': current_price,
            'stop_loss_price': current_price * 0.97,  # -3% au lieu de -5%
            'stop_loss_percent': -3.0,
            'tp1_price': current_price * 1.05,
            'tp1_percent': 5.0,
            'tp2_price': current_price * 1.10,
            'tp2_percent': 10.0,
            'tp3_price': current_price * 1.15,
            'tp3_percent': 15.0
        }

    # CAS 3b: TP atteint(s) + conditions favorables ‚Üí Nouveaux niveaux
    elif conditions_favorables:
        decision = "NOUVEAUX_NIVEAUX"
        raisons.append(f"‚úÖ {', '.join(tp_hit)} atteint(s)")
        for tp_name, gain in tp_gains.items():
            raisons.append(f"   {tp_name}: +{gain:.1f}%")
        raisons.append(f"üöÄ Conditions encore favorables ({decision_marche})")
        raisons.extend(raisons_marche['bullish'][:3])  # Top 3 raisons haussi√®res

        # Afficher type de pump
        if type_pump == "LENT":
            raisons.append(f"‚úÖ Pump sain ({velocite_pump:.1f}%/h) - Progression stable")

        # Calculer NOUVEAUX niveaux depuis le prix actuel
        # SL plus serr√© √† 95% (car d√©j√† en profit)
        nouveaux_niveaux = {
            'entry_price': current_price,
            'stop_loss_price': current_price * 0.95,
            'stop_loss_percent': -5.0,
            'tp1_price': current_price * 1.05,
            'tp1_percent': 5.0,
            'tp2_price': current_price * 1.10,
            'tp2_percent': 10.0,
            'tp3_price': current_price * 1.15,
            'tp3_percent': 15.0
        }

    # CAS 4: TP atteint(s) + conditions neutres/baissi√®res ‚Üí S√©curiser
    else:
        decision = "SECURISER_HOLD"
        raisons.append(f"‚úÖ {', '.join(tp_hit)} atteint(s)")
        for tp_name, gain in tp_gains.items():
            raisons.append(f"   {tp_name}: +{gain:.1f}%")
        raisons.append(f"‚ö†Ô∏è Conditions actuelles: {decision_marche}")
        raisons.extend(raisons_marche['bearish'][:2])  # Top 2 raisons baissi√®res
        raisons.append("üí° Trailing stop √† -5% recommand√© pour s√©curiser")

    return {
        'decision': decision,
        'tp_hit': tp_hit,
        'tp_gains': tp_gains,
        'prix_trop_eleve': prix_trop_eleve,
        'conditions_favorables': conditions_favorables,
        'raisons': raisons,
        'nouveaux_niveaux': nouveaux_niveaux,
        'hausse_depuis_alerte': hausse_depuis_alerte,
        'velocite_pump': velocite_pump,
        'type_pump': type_pump,
        'temps_ecoule_heures': temps_ecoule_heures
    }

# ============================================
# G√âN√âRATION ALERTE COMPL√àTE
# ============================================
def format_price(price: float) -> str:
    """
    Formate le prix de mani√®re intelligente et coh√©rente (FIX HARMONISATION):
    - Prix >= $1: 2 d√©cimales (ex: $1.23)
    - Prix >= $0.01: 4 d√©cimales (ex: $0.1574) - √âVITE PROBL√àMES ARRONDI TP
    - Prix < $0.01: 8 d√©cimales (ex: $0.00012345)
    """
    if price >= 1.0:
        return f"${price:.2f}"
    elif price >= 0.01:
        # 4 d√©cimales pour pr√©cision TP (√©vite arrondi $0.1574 ‚Üí $0.16)
        return f"${price:.4f}"
    else:
        # Pour les tr√®s petits prix, garder 8 d√©cimales
        return f"${price:.8f}"

def generer_alerte_complete(pool_data: Dict, score: int, base_score: int, momentum_bonus: int,
                            momentum: Dict, multi_pool_data: Dict, signals: List[str],
                            resistance_data: Dict, whale_analysis: Dict = None, is_first_alert: bool = True,
                            tracker: 'AlertTracker' = None) -> tuple:
    """G√©n√®re alerte ultra-compl√®te avec toutes les donn√©es.

    Args:
        tracker: Instance d'AlertTracker pour acc√©der √† l'historique (optionnel)

    Returns:
        tuple: (message_texte, donnees_regle5_dict)
    """

    # Initialiser les donn√©es R√àGLE 5 par d√©faut
    regle5_data = {
        'velocite_pump': 0,
        'type_pump': 'UNKNOWN',
        'decision_tp_tracking': None,
        'temps_depuis_alerte_precedente': 0,
        'is_alerte_suivante': 0
    }

    name = pool_data["name"]
    base_token = pool_data["base_token_name"]
    price = pool_data["price_usd"]
    vol_24h = pool_data["volume_24h"]
    vol_6h = pool_data["volume_6h"]
    vol_1h = pool_data["volume_1h"]
    liq = pool_data["liquidity"]
    pct_24h = pool_data["price_change_24h"]
    pct_6h = pool_data["price_change_6h"]
    pct_3h = pool_data["price_change_3h"]
    pct_1h = pool_data["price_change_1h"]
    age = pool_data["age_hours"]
    txns = pool_data["total_txns"]
    buys = pool_data["buys_24h"]
    sells = pool_data["sells_24h"]
    buys_1h = pool_data["buys_1h"]
    sells_1h = pool_data["sells_1h"]
    network_id = pool_data["network"]  # ID original pour le lien
    network_display = get_network_display_name(network_id)  # Nom lisible pour affichage
    ratio_vol_liq = (vol_24h / liq * 100) if liq > 0 else 0
    buy_ratio_24h = buys / sells if sells > 0 else 1.0
    buy_ratio_1h = buys_1h / sells_1h if sells_1h > 0 else 1.0

    # Initialiser les signaux volume (seront d√©finis dans l'analyse volume)
    signal_1h = None
    signal_6h = None

    # Emojis score
    if score >= 80:
        score_emoji = "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è"
        score_label = "EXCELLENT"
    elif score >= 70:
        score_emoji = "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è"
        score_label = "TR√àS BON"
    elif score >= 60:
        score_emoji = "‚≠êÔ∏è‚≠êÔ∏è"
        score_label = "BON"
    elif score >= 50:
        score_emoji = "‚≠êÔ∏è"
        score_label = "MOYEN"
    else:
        score_emoji = ""
        score_label = "FAIBLE"

    # ========== CONSTRUCTION ALERTE ==========
    # Titre diff√©rent selon s'il s'agit de la premi√®re alerte ou d'une mise √† jour
    if is_first_alert:
        txt = f"\nüÜï *Nouvelle opportunit√© sur le token {base_token}*\n"
    else:
        txt = f"\nüîÑ *Nouvelle analyse sur le token {base_token}*\n"
    txt += f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
    txt += f"üíé {name}\n"
    txt += f"‚õìÔ∏è Blockchain: {network_display}\n\n"

    # SCORE + CONFIANCE (NOUVEAU)
    confidence = calculate_confidence_score(pool_data)
    txt += f"üéØ *SCORE: {score}/100 {score_emoji} {score_label}*\n"
    txt += f"   Base: {base_score} | Momentum: {momentum_bonus:+d}"

    # Afficher whale score si disponible
    if whale_analysis:
        whale_score = whale_analysis['whale_score']
        if whale_score != 0:
            txt += f" | Whale: {whale_score:+d}"

    txt += f"\nüìä Confiance: {confidence}% (fiabilit√© donn√©es)\n"

    # ===== V3: TIER DE CONFIANCE BACKTEST =====
    tier = calculate_confidence_tier(pool_data)
    tier_emojis = {
        "ULTRA_HIGH": "üíéüíéüíé",
        "HIGH": "üíéüíé",
        "MEDIUM": "üíé",
        "LOW": "‚ö™",
        "VERY_LOW": "‚ö´"
    }
    tier_labels = {
        "ULTRA_HIGH": "ULTRA HIGH (Watchlist - 77-100% WR historique)",
        "HIGH": "HIGH (35-50% WR attendu)",
        "MEDIUM": "MEDIUM (25-30% WR attendu)",
        "LOW": "LOW (15-20% WR attendu)",
        "VERY_LOW": "VERY LOW (<15% WR attendu)"
    }
    tier_emoji = tier_emojis.get(tier, "‚ö™")
    tier_label = tier_labels.get(tier, "UNKNOWN")

    txt += f"üéñÔ∏è *TIER V3: {tier_emoji} {tier_label}*\n"

    # Afficher les raisons de filtrage V3 (si disponibles)
    v3_reasons = pool_data.get('v3_filter_reasons', [])
    if v3_reasons:
        # Afficher seulement les raisons positives (succ√®s)
        positive_reasons = [r.replace('‚úì ', '') for r in v3_reasons if r.startswith('‚úì')]
        if positive_reasons:
            txt += f"   V3 Checks: {' | '.join(positive_reasons[:3])}\n"  # Max 3 raisons

    txt += "\n"

    # NOUVEAU: Section WHALE ACTIVITY (FIX BUG #5 - Toujours afficher si whale_score != 0)
    if whale_analysis:
        whale_score_val = whale_analysis.get('whale_score', 0)
        pattern = whale_analysis.get('pattern', 'NORMAL')
        signals = whale_analysis.get('signals', [])

        # Afficher si whale_score != 0 OU si pattern != NORMAL OU si signals non vide
        if whale_score_val != 0 or pattern != 'NORMAL' or signals:
            concentration_risk = whale_analysis['concentration_risk']
            buyers_1h = whale_analysis['buyers_1h']
            sellers_1h = whale_analysis['sellers_1h']
            avg_buys = whale_analysis['avg_buys_per_buyer']
            avg_sells = whale_analysis.get('avg_sells_per_seller', 0)

            # Emoji selon pattern
            if pattern == 'WHALE_MANIPULATION':
                pattern_emoji = "üêã"
                pattern_label = "WHALE MANIPULATION"
            elif pattern == 'WHALE_SELLING':
                pattern_emoji = "üö®"
                pattern_label = "WHALE SELLING"
            elif pattern == 'DISTRIBUTED_BUYING':
                pattern_emoji = "‚úÖ"
                pattern_label = "ACCUMULATION DISTRIBU√âE"
            elif pattern == 'DISTRIBUTED_SELLING':
                pattern_emoji = "‚ö†Ô∏è"
                pattern_label = "SELLING PRESSURE"
            else:
                # Pattern NORMAL mais whale_score != 0 (ex: concentration 24h)
                pattern_emoji = "üìä"
                pattern_label = "WHALE ACTIVITY"

            txt += f"\n{pattern_emoji} *{pattern_label}*\n"
            txt += f"   Buyers: {buyers_1h} | Sellers: {sellers_1h}\n"
            txt += f"   Avg buys/buyer: {avg_buys:.1f}x"
            if avg_sells > 0:
                txt += f" | Avg sells/seller: {avg_sells:.1f}x"
            txt += f"\n   Risque concentration: {concentration_risk}\n"

            # Afficher les signaux si disponibles
            if signals:
                txt += f"   Signaux: {', '.join(signals[:2])}\n"

    txt += "\n"

    # ========== ANALYSE TP TRACKING (pour alertes suivantes) ==========
    if not is_first_alert and tracker is not None:
        token_address = pool_data.get("pool_address", "")
        previous_alert = tracker.get_last_alert_for_token(token_address)

        if previous_alert:
            # Pr√©-calculer les signaux volume pour l'analyse
            vol_24h_avg = vol_24h / 24
            vol_6h_avg = vol_6h / 6 if vol_6h > 0 else 0
            ratio_1h_vs_6h = (vol_1h / vol_6h_avg) if vol_6h_avg > 0 else 0
            ratio_6h_vs_24h = (vol_6h_avg / vol_24h_avg) if vol_24h_avg > 0 else 0

            # D√©terminer signaux
            if ratio_1h_vs_6h >= 2.0:
                signal_1h = "FORTE_ACCELERATION"
            elif ratio_1h_vs_6h >= 1.5:
                signal_1h = "ACCELERATION"
            elif ratio_1h_vs_6h <= 0.3:
                signal_1h = "FORT_RALENTISSEMENT"
            elif ratio_1h_vs_6h <= 0.5:
                signal_1h = "RALENTISSEMENT"
            else:
                signal_1h = "STABLE"

            if ratio_6h_vs_24h >= 1.8:
                signal_6h = "PUMP_EN_COURS"
            elif ratio_6h_vs_24h >= 1.3:
                signal_6h = "HAUSSE_PROGRESSIVE"
            elif ratio_6h_vs_24h <= 0.7:
                signal_6h = "BAISSE_TENDANCIELLE"
            else:
                signal_6h = "STABLE"

            # Analyser TP tracking (passer le tracker pour v√©rifier le prix MAX atteint)
            analyse_tp = analyser_alerte_suivante(
                previous_alert, price, pool_data, score, momentum, signal_1h, signal_6h, tracker
            )
            log(f"   üîç DEBUG RETOUR analyser_alerte_suivante: decision={analyse_tp.get('decision') if analyse_tp else None}, type={type(analyse_tp)}")

            # VALIDATION: V√©rifier que analyse_tp est un dict valide
            if not analyse_tp or not isinstance(analyse_tp, dict):
                log(f"   ‚ö†Ô∏è analyse_tp invalide: {type(analyse_tp)}")
                # Ne pas afficher la section TP tracking si erreur
            elif analyse_tp['decision'] == 'ERROR':
                # V√©rifier si l'analyse a √©chou√© (decision == 'ERROR')
                log(f"   ‚ö†Ô∏è Analyse TP tracking √©chou√©e, skip section suivi")
                # Ne pas afficher la section TP tracking si erreur
            else:
                # Mettre √† jour les donn√©es R√àGLE 5
                regle5_data = {
                    'velocite_pump': analyse_tp['velocite_pump'],
                    'type_pump': analyse_tp['type_pump'],
                    'decision_tp_tracking': analyse_tp['decision'],
                    'temps_depuis_alerte_precedente': analyse_tp['temps_ecoule_heures'],
                    'is_alerte_suivante': 1
                }

                # Afficher section TP TRACKING
                txt += f"‚îÅ‚îÅ‚îÅ SUIVI ALERTE PR√âC√âDENTE ‚îÅ‚îÅ‚îÅ\n"
                entry_prev = previous_alert.get('entry_price', previous_alert.get('price_at_alert', 0))
                txt += f"üìç Entry pr√©c√©dente: {format_price(entry_prev)}\n"
                txt += f"üí∞ Prix actuel: {format_price(price)} ({analyse_tp['hausse_depuis_alerte']:+.1f}%)\n"

                # Afficher v√©locit√© du pump
                temps_h = analyse_tp['temps_ecoule_heures']
                velocite = analyse_tp['velocite_pump']
                type_pump = analyse_tp['type_pump']

                if temps_h < 1:
                    temps_display = f"{temps_h * 60:.0f} min"
                else:
                    temps_display = f"{temps_h:.1f}h"

                # Emoji selon type de pump
                if type_pump == "PARABOLIQUE":
                    pump_emoji = "üö®"
                    pump_label = "PARABOLIQUE (DANGER)"
                elif type_pump == "TRES_RAPIDE":
                    pump_emoji = "‚ö°"
                    pump_label = "TR√àS RAPIDE"
                elif type_pump == "RAPIDE":
                    pump_emoji = "üî•"
                    pump_label = "RAPIDE"
                elif type_pump == "NORMAL":
                    pump_emoji = "üìà"
                    pump_label = "NORMAL"
                else:  # LENT
                    pump_emoji = "‚úÖ"
                    pump_label = "SAIN"

                txt += f"‚è±Ô∏è Temps √©coul√©: {temps_display} | {pump_emoji} V√©locit√©: {velocite:.0f}%/h ({pump_label})\n"

                # Afficher Prix MAX atteint (CRITIQUE pour comprendre d√©tection TP)
                if tracker is not None and 'previous_alert' in locals() and previous_alert:
                    alert_id = previous_alert.get('id', 0)
                    prix_max_db = tracker.get_highest_price_for_alert(alert_id) if alert_id > 0 else None
                    prix_max_display = max(prix_max_db or 0, price)

                    if prix_max_display > 0:
                        entry_price_ref = previous_alert.get('entry_price', price)
                        gain_max = ((prix_max_display - entry_price_ref) / entry_price_ref) * 100
                        txt += f"üìà Prix MAX atteint: {format_price(prix_max_display)} (+{gain_max:.1f}%)\n"

                # Afficher TP atteints (bas√© sur Prix MAX, pas prix actuel)
                if analyse_tp['tp_hit']:
                    txt += f"‚úÖ *TP ATTEINTS:* {', '.join(analyse_tp['tp_hit'])}\n"
                    for tp_name, gain in analyse_tp['tp_gains'].items():
                        txt += f"   {tp_name}: +{gain:.1f}%\n"
                else:
                    txt += f"‚è≥ Aucun TP atteint pour le moment\n"

                txt += f"\nüéØ *D√âCISION: {analyse_tp['decision']}*\n"

                # Afficher raisons
                for raison in analyse_tp['raisons']:
                    txt += f"{raison}\n"

                txt += "\n"

    # PRIX & MOMENTUM
    txt += f"‚îÅ‚îÅ‚îÅ PRIX & MOMENTUM ‚îÅ‚îÅ‚îÅ\n"
    txt += f"üí∞ Prix: {format_price(price)}\n"

    # Multi-timeframe avec analyse de tendance
    txt += f"üìä "
    txt += f"24h: {pct_24h:+.1f}% "
    if pct_6h != 0:
        txt += f"| 6h: {pct_6h:+.1f}% "
    if pct_3h != 0:
        txt += f"| 3h: {pct_3h:+.1f}% "
    if pct_1h != 0:
        emoji_1h = "üöÄ" if pct_1h > 5 else ("üü¢" if pct_1h > 0 else "üî¥")
        txt += f"| 1h: {pct_1h:+.1f}% {emoji_1h}"
    txt += "\n"

    # Analyse de la structure de tendance (NOUVEAU)
    if pct_6h != 0 and pct_3h != 0 and pct_1h != 0:
        # D√©terminer si acc√©l√©ration haussi√®re ou essoufflement
        if pct_1h > pct_3h > pct_6h and pct_1h > 0:
            txt += f"üìà Tendance: ACC√âL√âRATION HAUSSI√àRE üî•\n"
        elif pct_6h > pct_3h > pct_1h and pct_6h > 0:
            txt += f"‚ö†Ô∏è Tendance: ESSOUFFLEMENT (sortie proche) üìâ\n"
        elif pct_1h < 0 < pct_3h < pct_6h:
            txt += f"üîÑ Tendance: REPRISE apr√®s correction (bon entry) ‚úÖ\n"
        elif pct_1h < pct_3h < pct_6h and pct_1h < 0:
            txt += f"üî¥ Tendance: D√âC√âL√âRATION BAISSI√àRE ‚ö†Ô∏è\n"

    # R√©sistance
    if resistance_data and resistance_data.get("resistance"):
        txt += f"üéØ R√©sistance: {format_price(resistance_data['resistance'])} "
        txt += f"(+{resistance_data['resistance_dist_pct']:.1f}%)\n"

    txt += "\n"

    # ACTIVIT√â
    txt += f"‚îÅ‚îÅ‚îÅ ACTIVIT√â ‚îÅ‚îÅ‚îÅ\n"
    txt += f"üìä Vol 24h: ${vol_24h/1000:.0f}K\n"

    # Analyse de l'√©volution du volume MULTI-NIVEAUX (NOUVEAU)
    if vol_24h > 0 and vol_6h > 0 and vol_1h > 0:
        # Calculer les moyennes horaires
        vol_24h_avg = vol_24h / 24  # Volume moyen par heure sur 24h
        vol_6h_avg = vol_6h / 6     # Volume moyen par heure sur 6h

        # Ratios d'acc√©l√©ration
        ratio_1h_vs_6h = (vol_1h / vol_6h_avg) if vol_6h_avg > 0 else 0
        ratio_6h_vs_24h = (vol_6h_avg / vol_24h_avg) if vol_24h_avg > 0 else 0

        # ANALYSE DOUBLE NIVEAU pour d√©tecter les meilleurs setups

        # Niveau 1: Court terme (1h vs 6h)
        if ratio_1h_vs_6h >= 2.0:
            signal_1h = "FORTE_ACCELERATION"
            emoji_1h = "üî•"
            text_1h = f"FORTE ACC√âL√âRATION ({ratio_1h_vs_6h:.1f}x)"
        elif ratio_1h_vs_6h >= 1.5:
            signal_1h = "ACCELERATION"
            emoji_1h = "üìà"
            text_1h = f"ACC√âL√âRATION ({ratio_1h_vs_6h:.1f}x)"
        elif ratio_1h_vs_6h <= 0.5:
            signal_1h = "RALENTISSEMENT"
            emoji_1h = "‚ö†Ô∏è"
            text_1h = f"RALENTISSEMENT ({ratio_1h_vs_6h:.1f}x)"
        elif ratio_1h_vs_6h <= 0.3:
            signal_1h = "FORT_RALENTISSEMENT"
            emoji_1h = "üî¥"
            text_1h = f"FORT RALENTISSEMENT ({ratio_1h_vs_6h:.1f}x)"
        else:
            signal_1h = "STABLE"
            emoji_1h = "‚û°Ô∏è"
            text_1h = f"STABLE ({ratio_1h_vs_6h:.1f}x)"

        # Niveau 2: Moyen terme (6h vs 24h)
        if ratio_6h_vs_24h >= 1.8:
            signal_6h = "PUMP_EN_COURS"
            emoji_6h = "üöÄ"
            text_6h = f"Pump en cours ({ratio_6h_vs_24h:.1f}x)"
        elif ratio_6h_vs_24h >= 1.3:
            signal_6h = "HAUSSE_PROGRESSIVE"
            emoji_6h = "üìä"
            text_6h = f"Hausse progressive ({ratio_6h_vs_24h:.1f}x)"
        elif ratio_6h_vs_24h <= 0.7:
            signal_6h = "BAISSE_TENDANCIELLE"
            emoji_6h = "üìâ"
            text_6h = f"Baisse tendancielle ({ratio_6h_vs_24h:.1f}x)"
        else:
            signal_6h = "STABLE"
            emoji_6h = "‚û°Ô∏è"
            text_6h = f"Normal ({ratio_6h_vs_24h:.1f}x)"

        # VERDICT FINAL combinant les deux niveaux
        txt += f"üìä Volume Multi-Timeframe:\n"
        txt += f"   Court terme (1h): {emoji_1h} {text_1h}\n"
        txt += f"   Moyen terme (6h): {emoji_6h} {text_6h}\n"

        # PATTERN GAGNANTS (bas√© sur backtest)
        if signal_1h in ["FORTE_ACCELERATION", "ACCELERATION"] and signal_6h == "PUMP_EN_COURS":
            txt += f"‚úÖ PATTERN: ENTR√âE ID√âALE - Pump actif + acc√©l√©ration r√©cente üéØ\n"
        elif signal_1h == "FORTE_ACCELERATION" and signal_6h in ["HAUSSE_PROGRESSIVE", "STABLE"]:
            txt += f"‚úÖ PATTERN: BON ENTRY - Nouveau pump qui d√©marre üü¢\n"
        elif signal_1h in ["RALENTISSEMENT", "FORT_RALENTISSEMENT"] and signal_6h == "PUMP_EN_COURS":
            txt += f"‚ö†Ô∏è PATTERN: SORTIE PROCHE - Volume qui faiblit (essoufflement) üö™\n"
        elif signal_1h == "STABLE" and signal_6h == "PUMP_EN_COURS":
            txt += f"‚è∏Ô∏è PATTERN: CONSOLIDATION - Pause avant continuation possible\n"
        elif signal_1h in ["RALENTISSEMENT", "FORT_RALENTISSEMENT"]:
            txt += f"üî¥ PATTERN: √âVITER - Volume en chute libre ‚ùå\n"

        # Afficher d√©tails volumes
        txt += f"   Vol: 24h ${vol_24h/1000:.0f}K | 6h ${vol_6h/1000:.0f}K | 1h ${vol_1h/1000:.0f}K\n"

    # Volume spike ?
    elif vol_1h > 0:
        vol_1h_normalized = vol_1h * 24
        if vol_1h_normalized > vol_24h * 1.3:
            spike = ((vol_1h_normalized / vol_24h) - 1) * 100
            txt += f"‚ö° Vol 1h: ${vol_1h/1000:.0f}K (x{vol_1h_normalized/vol_24h:.1f} activit√©!) üî•\n"
        else:
            txt += f"üìâ Vol 1h: ${vol_1h/1000:.0f}K\n"

    txt += f"üíß Liquidit√©: ${liq/1000:.0f}K\n"

    # Transactions 24h - Format explicite avec estimation traders (NOUVEAU)
    txt += f"üîÑ Transactions 24h: {txns}\n"
    # Estimation traders: moyenne 2-3 tx par trader
    traders_estimate = int(txns / 2.5)  # Estimation conservative
    txt += f"üë• Traders estim√©s: ~{traders_estimate} (bas√© sur txns)\n"
    buys_pct = (buys / txns * 100) if txns > 0 else 0
    sells_pct = (sells / txns * 100) if txns > 0 else 0
    txt += f"   üü¢ ACHATS: {buys} ({buys_pct:.0f}%)\n"
    txt += f"   üî¥ VENTES: {sells} ({sells_pct:.0f}%)\n"

    # Pression dominante
    if buy_ratio_24h >= 1.0:
        txt += f"   ‚öñÔ∏è Pression: ACHETEURS dominent (ratio {buy_ratio_24h:.2f})\n"
    elif buy_ratio_24h >= 0.8:
        txt += f"   ‚öñÔ∏è Pression: √âQUILIBR√âE (ratio {buy_ratio_24h:.2f})\n"
    else:
        txt += f"   ‚öñÔ∏è Pression: VENDEURS dominent (ratio {buy_ratio_24h:.2f})\n"

    # Pression 1h (si diff√©rente)
    if buys_1h > 0 and sells_1h > 0 and abs(buy_ratio_1h - buy_ratio_24h) > 0.1:
        txt += f"\nüìä Pression 1h:\n"
        buys_1h_pct = (buys_1h / (buys_1h + sells_1h) * 100)
        sells_1h_pct = (sells_1h / (buys_1h + sells_1h) * 100)
        txt += f"   üü¢ ACHATS: {buys_1h} ({buys_1h_pct:.0f}%)"

        if buy_ratio_1h > buy_ratio_24h:
            txt += f" ‚¨ÜÔ∏è\n"
        else:
            txt += f" ‚¨áÔ∏è\n"

        txt += f"   üî¥ VENTES: {sells_1h} ({sells_1h_pct:.0f}%)"

        if buy_ratio_1h > buy_ratio_24h:
            txt += f" ‚¨áÔ∏è\n"
        else:
            txt += f" ‚¨ÜÔ∏è\n"

        # Analyse de la tendance de pression (focus sur l'√©volution, pas les absolus)
        ratio_change = buy_ratio_1h - buy_ratio_24h

        if ratio_change > 0.1:  # Pression acheteuse augmente significativement
            txt += f"   ‚úÖ ACHETEURS prennent le contr√¥le ! (+{ratio_change:.1%})\n"
        elif ratio_change < -0.1:  # Pression vendeuse augmente significativement
            txt += f"   ‚ö†Ô∏è VENDEURS prennent le contr√¥le ! ({ratio_change:.1%})\n"
        else:  # Pression stable
            if buy_ratio_1h >= 0.75:
                txt += f"   ‚û°Ô∏è Pression ACHETEUSE stable ({buy_ratio_1h:.0%})\n"
            elif buy_ratio_1h <= 0.55:
                txt += f"   ‚û°Ô∏è Pression VENDEUSE stable ({buy_ratio_1h:.0%})\n"
            else:
                txt += f"   ‚û°Ô∏è √âquilibre acheteurs/vendeurs ({buy_ratio_1h:.0%})\n"

    txt += f"\n‚ö° Vol/Liq: {ratio_vol_liq:.0f}%\n"
    txt += f"‚è∞ Cr√©√© il y a {age:.0f}h\n\n"

    # MULTI-POOL (si applicable)
    if multi_pool_data.get("is_multi_pool"):
        txt += f"‚îÅ‚îÅ‚îÅ MULTI-POOL ‚îÅ‚îÅ‚îÅ\n"
        txt += f"üåê Pools actifs: {multi_pool_data['num_pools']}\n"
        txt += f"üìä Volume total: ${multi_pool_data['total_volume']/1000:.0f}K\n"
        txt += f"üíß Liquidit√© totale: ${multi_pool_data['total_liquidity']/1000:.0f}K\n"

        # D√©tail pools
        for activity in multi_pool_data['pool_activities']:
            txt += f"   ‚Ä¢ {activity['pair']}: {activity['vol_liq_pct']:.0f}% Vol/Liq\n"

        if multi_pool_data.get("is_weth_dominant"):
            txt += f"‚ö° WETH pool dominant = Smart money üöÄ\n"
        txt += "\n"

    # SIGNAUX
    if signals:
        txt += f"‚îÅ‚îÅ‚îÅ SIGNAUX D√âTECT√âS ‚îÅ‚îÅ‚îÅ\n"
        for signal in signals:
            txt += f"{signal}\n"
        txt += "\n"

    # √âVALUATION DES CONDITIONS MARCH√â POUR D√âCISION D'ENTR√âE
    should_enter, decision, analysis_reasons = evaluer_conditions_marche(
        pool_data, score, momentum, signal_1h, signal_6h
    )

    # ACTION RECOMMAND√âE - CONDITIONNELLE
    txt += f"‚îÅ‚îÅ‚îÅ ACTION RECOMMAND√âE ‚îÅ‚îÅ‚îÅ\n"

    # V√©rifier si on a une analyse TP avec nouveaux niveaux (alerte suivante)
    show_nouveaux_niveaux = (not is_first_alert and tracker is not None and
                             'analyse_tp' in locals() and
                             analyse_tp['decision'] == "NOUVEAUX_NIVEAUX")

    if show_nouveaux_niveaux:
        # ‚úÖ NOUVEAUX NIVEAUX TP/SL (car TP pr√©c√©dents atteints + conditions favorables)
        txt += f"üöÄ NOUVEAUX NIVEAUX - TP pr√©c√©dents atteints !\n\n"

        nouveaux = analyse_tp['nouveaux_niveaux']
        entry_new = nouveaux['entry_price']
        stop_loss_new = nouveaux['stop_loss_price']
        tp1_new = nouveaux['tp1_price']
        tp2_new = nouveaux['tp2_price']
        tp3_new = nouveaux['tp3_price']

        txt += f"‚ö° Entry: {format_price(entry_new)} üéØ\n"
        txt += f"üìç Limite entr√©e: {format_price(entry_new * 1.03)} (max +3%)\n"
        txt += f"üõë Stop loss: {format_price(stop_loss_new)} (-5%) ‚ö° SL SERR√â\n"
        txt += f"üéØ TP1 (50%): {format_price(tp1_new)} (+5%)\n"
        txt += f"üéØ TP2 (30%): {format_price(tp2_new)} (+10%)\n"
        txt += f"üéØ TP3 (20%): {format_price(tp3_new)} (+15%)\n"
        txt += f"üîÑ Trail stop: -5% apr√®s TP1\n\n"

        txt += f"üí° NOTE: SL plus serr√© (-5%) car d√©j√† en profit !\n\n"

    elif should_enter and decision == "BUY":
        # ‚úÖ CONDITIONS FAVORABLES - Afficher Entry/SL/TP
        txt += f"‚úÖ SIGNAL D'ENTR√âE VALID√â\n\n"

        # Afficher les raisons bullish
        if analysis_reasons['bullish']:
            txt += f"üìà Signaux haussiers:\n"
            for reason in analysis_reasons['bullish']:
                txt += f"   ‚Ä¢ {reason}\n"
            txt += "\n"

        # FIX COH√âRENCE TP: Si alerte suivante, utiliser TP de l'alerte ORIGINALE
        if not is_first_alert and tracker is not None and 'previous_alert' in locals() and previous_alert:
            # Utiliser les TP de la premi√®re alerte (COH√âRENCE)
            entry_original = previous_alert.get('entry_price', price)
            sl_original = previous_alert.get('stop_loss_price', price * 0.90)
            tp1_original = previous_alert.get('tp1_price', price * 1.05)
            tp2_original = previous_alert.get('tp2_price', price * 1.10)
            tp3_original = previous_alert.get('tp3_price', price * 1.15)

            txt += f"‚ö° Entry (alerte initiale): {format_price(entry_original)} üéØ\n"
            txt += f"üìç Limite entr√©e: {format_price(entry_original * 1.03)} (max +3%)\n"
            txt += f"üõë Stop loss: {format_price(sl_original)} (-10%)\n"
            txt += f"üéØ TP1 (50%): {format_price(tp1_original)} (+5%)\n"
            txt += f"üéØ TP2 (30%): {format_price(tp2_original)} (+10%)\n"
            txt += f"üéØ TP3 (20%): {format_price(tp3_original)} (+15%)\n"
            txt += f"üîÑ Trail stop: -5% apr√®s TP1\n\n"
        else:
            # Premi√®re alerte: calculer nouveaux TP depuis prix actuel
            price_max_entry = price * 1.03
            txt += f"‚ö° Entry: {format_price(price)} üéØ\n"
            txt += f"üìç Limite entr√©e: {format_price(price_max_entry)} (max +3%)\n"

            # Stop loss
            stop_loss = price * 0.90
            txt += f"üõë Stop loss: {format_price(stop_loss)} (-10%)\n"

            # Take profits
            tp1 = price * 1.05
            tp2 = price * 1.10
            tp3 = price * 1.15
            txt += f"üéØ TP1 (50%): {format_price(tp1)} (+5%)\n"
            txt += f"üéØ TP2 (30%): {format_price(tp2)} (+10%)\n"
            txt += f"üéØ TP3 (20%): {format_price(tp3)} (+15%)\n"
            txt += f"üîÑ Trail stop: -5% apr√®s TP1\n\n"

    elif decision == "WAIT":
        # ‚è∏Ô∏è CONDITIONS INCERTAINES - Attendre
        txt += f"‚è∏Ô∏è ATTENDRE - Conditions pas encore id√©ales\n\n"

        # Afficher les raisons
        if analysis_reasons['bearish']:
            txt += f"‚ö†Ô∏è Signaux n√©gatifs d√©tect√©s:\n"
            for reason in analysis_reasons['bearish']:
                txt += f"   ‚Ä¢ {reason}\n"
            txt += "\n"

        if analysis_reasons['bullish']:
            txt += f"‚úÖ Signaux positifs:\n"
            for reason in analysis_reasons['bullish']:
                txt += f"   ‚Ä¢ {reason}\n"
            txt += "\n"

        txt += f"üí° RECOMMANDATION:\n"
        txt += f"   ‚Ä¢ Surveiller l'√©volution du volume et du prix\n"
        txt += f"   ‚Ä¢ Attendre confirmation d'une tendance haussi√®re claire\n"
        txt += f"   ‚Ä¢ Entrer si le volume acc√©l√®re et la pression acheteuse augmente\n"
        txt += f"   ‚Ä¢ Risque mod√©r√© - Prudence recommand√©e\n\n"

    else:  # EXIT
        # üö´ CONDITIONS D√âFAVORABLES - Ne pas entrer / Sortir
        txt += f"üö´ PAS D'ENTR√âE - Sortie ou √©viter le march√©\n\n"

        # Afficher les raisons bearish
        if analysis_reasons['bearish']:
            txt += f"üî¥ Raisons de sortie/√©viter:\n"
            for reason in analysis_reasons['bearish']:
                txt += f"   ‚Ä¢ {reason}\n"
            txt += "\n"

        # Afficher les points positifs s'il y en a
        if analysis_reasons['bullish']:
            txt += f"‚ö†Ô∏è Points positifs (insuffisants):\n"
            for reason in analysis_reasons['bullish']:
                txt += f"   ‚Ä¢ {reason}\n"
            txt += "\n"

        txt += f"üí° RECOMMANDATION:\n"

        # D√©terminer si c'est un pump qui s'essouffle ou un token √† √©viter
        is_essoufflement = any("SORTIE" in r or "Essoufflement" in r or "D√©c√©l√©ration" in r for r in analysis_reasons['bearish'])
        is_volume_problem = any("√âVITER" in r or "RALENTISSEMENT" in r or "chute" in r for r in analysis_reasons['bearish'])

        if is_essoufflement:
            txt += f"   ‚Ä¢ ‚ö†Ô∏è SORTIR si vous √™tes en position (pump s'essouffle)\n"
            txt += f"   ‚Ä¢ NE PAS ENTRER - Momentum en d√©c√©l√©ration\n"
            txt += f"   ‚Ä¢ Attendre un √©ventuel rebound seulement si volume se stabilise\n"
            txt += f"   ‚Ä¢ Risque √©lev√© de correction\n"
        elif is_volume_problem:
            txt += f"   ‚Ä¢ üî¥ NE PAS ENTRER - Volume en chute\n"
            txt += f"   ‚Ä¢ √âviter ce token pour le moment\n"
            txt += f"   ‚Ä¢ Chercher d'autres opportunit√©s avec volume sain\n"
            txt += f"   ‚Ä¢ Rebound peu probable sans nouvelle impulsion\n"
        else:
            txt += f"   ‚Ä¢ üö´ Conditions actuelles d√©favorables\n"
            txt += f"   ‚Ä¢ NE PAS ENTRER tant que la situation ne s'am√©liore pas\n"
            txt += f"   ‚Ä¢ Surveiller pour un √©ventuel rebound avec:\n"
            txt += f"     - Reprise du volume\n"
            txt += f"     - Augmentation de la pression acheteuse\n"
            txt += f"     - Momentum redevenant positif\n"

        txt += "\n"

    # RISQUES
    txt += f"‚îÅ‚îÅ‚îÅ RISQUES ‚îÅ‚îÅ‚îÅ\n"
    if age < 24:
        txt += f"‚ö†Ô∏è Tr√®s jeune ({age:.0f}h) - Volatilit√© √©lev√©e\n"
    elif age > 72:
        txt += f"‚ö†Ô∏è Age: {age:.0f}h (exit window pass√©e?)\n"

    if pct_24h < -15:
        txt += f"‚ö†Ô∏è Variation 24h n√©gative ({pct_24h:.1f}%) - Risque re-dump\n"

    if liq >= 500000:
        txt += f"‚úÖ Liquidit√© solide (${liq/1000:.0f}K) - Faible risque rug\n"
    elif liq >= 200000:
        txt += f"‚ö†Ô∏è Liquidit√© moyenne (${liq/1000:.0f}K) - Prudence\n"
    else:
        txt += f"üö® Liquidit√© faible (${liq/1000:.0f}K) - Risque √©lev√©\n"

    txt += f"\nüîó https://geckoterminal.com/{network_id.lower()}/pools/{pool_data['pool_address']}\n"

    return txt, regle5_data

# ============================================
# SCAN PRINCIPAL
# ============================================
def scan_geckoterminal():
    """Scan GeckoTerminal avec analyse avanc√©e."""

    log("=" * 80)
    log("ü¶é GECKOTERMINAL SCANNER V3.1 - ULTRA_RENTABLE")
    log("=" * 80)

    all_pools = []

    # Collecter tous les pools
    for network in NETWORKS:
        log(f"\nüîç Scan r√©seau: {network.upper()}")

        # Trending pools - r√©cup√©rer 3 pages (60 pools au lieu de 20)
        trending_count = 0
        for page in range(1, 4):  # Pages 1, 2, 3
            trending = get_trending_pools(network, page=page)
            if trending:
                trending_count += len(trending)
                for pool in trending:
                    pool_data = parse_pool_data(pool, network)
                    if pool_data and pool_data["age_hours"] <= MAX_TOKEN_AGE_HOURS:
                        all_pools.append(pool_data)
            time.sleep(1)  # Petite pause entre les pages

        if trending_count > 0:
            log(f"   üìä {trending_count} pools trending trouv√©s (3 pages)")

        time.sleep(2)

        # New pools - r√©cup√©rer 3 pages (60 pools au lieu de 20)
        new_count = 0
        for page in range(1, 4):  # Pages 1, 2, 3
            new_pools = get_new_pools(network, page=page)
            if new_pools:
                new_count += len(new_pools)
                for pool in new_pools:
                    pool_data = parse_pool_data(pool, network)
                    if pool_data and pool_data["age_hours"] <= MAX_TOKEN_AGE_HOURS:
                        all_pools.append(pool_data)
            time.sleep(1)  # Petite pause entre les pages

        if new_count > 0:
            log(f"   üÜï {new_count} nouveaux pools trouv√©s (3 pages)")

        time.sleep(2)

    log(f"\nüìä Total pools collect√©s: {len(all_pools)}")

    # Mettre √† jour historique (seulement buy ratio)
    for pool_data in all_pools:
        update_buy_ratio_history(pool_data)

    # NOUVEAU: Mettre √† jour le prix MAX en temps r√©el pour TOUS les tokens track√©s
    # CRITIQUE pour backtesting : capture les pics de prix entre chaque scan
    if alert_tracker is not None:
        for pool_data in all_pools:
            token_address = pool_data.get('token_address')
            current_price = pool_data.get('price', 0)

            if token_address and current_price > 0:
                # V√©rifier si ce token a une alerte active
                previous_alert = alert_tracker.get_last_alert_for_token(token_address)
                if previous_alert:
                    alert_id = previous_alert.get('id')
                    # Mettre √† jour le prix MAX en DB
                    alert_tracker.update_price_max_realtime(alert_id, current_price)

    # Grouper par token
    grouped = group_pools_by_token(all_pools)

    log(f"üîó Tokens uniques d√©tect√©s: {len(grouped)}")

    # Analyser chaque token
    opportunities = []
    tokens_rejected = 0  # Initialiser ici pour √©viter UnboundLocalError

    for base_token, pools in grouped.items():
        # Multi-pool analysis
        multi_pool_data = analyze_multi_pool(pools)

        # Analyser chaque pool
        for pool_data in pools:
            # Momentum - SIMPLIFI√â: depuis API directement
            momentum = get_price_momentum_from_api(pool_data)

            # R√©sistance - SIMPLIFI√â: calcul basique
            resistance_data = find_resistance_simple(pool_data)

            # Score (avec analyse whale)
            score, base_score, momentum_bonus, whale_analysis = calculate_final_score(pool_data, momentum, multi_pool_data)

            # NOUVEAU: Rejeter imm√©diatement si WHALE DUMP d√©tect√©
            if whale_analysis['pattern'] == 'WHALE_SELLING':
                log(f"   üö® {pool_data['name']}: WHALE DUMP d√©tect√© - REJET√â")
                tokens_rejected += 1
                continue

            # Validation
            is_valid, reason = is_valid_opportunity(pool_data, score)

            if is_valid:
                # D√©tecter signaux
                signals = detect_signals(pool_data, momentum, multi_pool_data)

                # Ajouter signaux whale aux signaux existants
                if whale_analysis['signals']:
                    signals.extend(whale_analysis['signals'])

                opportunities.append({
                    "pool_data": pool_data,
                    "score": score,
                    "base_score": base_score,
                    "momentum_bonus": momentum_bonus,
                    "whale_analysis": whale_analysis,  # NOUVEAU
                    "momentum": momentum,
                    "multi_pool_data": multi_pool_data,
                    "signals": signals,
                    "resistance_data": resistance_data,
                })

                log(f"   ‚úÖ Opportunit√©: {pool_data['name']} (Score: {score})")
            else:
                log(f"   ‚è≠Ô∏è  {pool_data['name']}: {reason}")

    # Trier par score
    opportunities.sort(key=lambda x: x["score"], reverse=True)

    log(f"\nüìä TOTAL: {len(opportunities)} opportunit√©s d√©tect√©es")

    # Envoyer alertes
    alerts_sent = 0
    # tokens_rejected d√©j√† initialis√© ligne 2078

    for opp in opportunities:
        base_token = opp["pool_data"]["base_token_name"]
        pool_addr = opp["pool_data"]["pool_address"]
        alert_key = f"{base_token}_{pool_addr}"

        # ==========================================
        # V√âRIFICATION DE S√âCURIT√â
        # ==========================================
        # Utiliser pool_address comme token_address (c'est l'adresse du pool/token)
        token_address = opp["pool_data"]["pool_address"]
        network = opp["pool_data"]["network"]

        log(f"\nüîí V√©rification s√©curit√©: {opp['pool_data']['name']}")

        security_result = security_checker.check_token_security(token_address, network)

        # V√©rifier si le token passe les crit√®res de s√©curit√©
        should_send, reason = security_checker.should_send_alert(security_result, min_security_score=50)

        if not should_send:
            log(f"‚õî Token rejet√©: {reason}")
            log(f"   Score s√©curit√©: {security_result['security_score']}/100")
            log(f"   Niveau risque: {security_result['risk_level']}")
            tokens_rejected += 1
            continue

        log(f"‚úÖ S√©curit√© valid√©e (Score: {security_result['security_score']}/100)")

        # ==========================================
        # ENVOI DE L'ALERTE (apr√®s validation s√©curit√©)
        # ==========================================

        # V√©rifier si c'est la premi√®re alerte pour ce token
        is_first_alert = not alert_tracker.token_already_alerted(token_address)

        # G√©n√©rer le message d'alerte (pour r√©cup√©rer regle5_data)
        alert_msg, regle5_data = generer_alerte_complete(
            opp["pool_data"],
            opp["score"],
            opp["base_score"],
            opp["momentum_bonus"],
            opp["momentum"],
            opp["multi_pool_data"],
            opp["signals"],
            opp["resistance_data"],
            opp.get("whale_analysis"),  # NOUVEAU: Passer analyse whale
            is_first_alert,
            alert_tracker  # Passer le tracker pour l'analyse TP
        )

        # NOUVEAU: V√©rifier si on doit envoyer l'alerte (FIX BUG #1 - SPAM)
        price = opp["pool_data"].get("price_usd", 0)
        should_send, send_reason = should_send_alert(token_address, price, alert_tracker, regle5_data)

        if not should_send:
            log(f"‚è∏Ô∏è Alerte bloqu√©e (anti-spam): {opp['pool_data']['name']}")
            log(f"   Raison: {send_reason}")
            continue

        # Legacy cooldown check (pour compatibilit√©)
        if check_cooldown(alert_key):
            # Ajouter les infos de s√©curit√© √† l'alerte
            security_info = security_checker.format_security_warning(security_result)
            alert_msg = alert_msg + "\n" + security_info

            if send_telegram(alert_msg):
                log(f"‚úÖ Alerte envoy√©e: {opp['pool_data']['name']} (Score: {opp['score']})")

                # ==========================================
                # SAUVEGARDE EN BASE DE DONN√âES + TRACKING AUTO
                # ==========================================
                try:
                    # Pr√©parer les donn√©es pour la DB
                    price = opp["pool_data"].get("price_usd", 0)
                    entry_price = price
                    stop_loss_price = price * 0.90  # -10%
                    tp1_price = price * 1.05  # +5%
                    tp2_price = price * 1.10  # +10%
                    tp3_price = price * 1.15  # +15%

                    alert_data = {
                        'token_name': opp["pool_data"]["name"],
                        'token_address': token_address,
                        'network': network,
                        'price_at_alert': price,
                        'score': opp["score"],
                        'base_score': opp["base_score"],
                        'momentum_bonus': opp["momentum_bonus"],
                        'confidence_score': security_result['security_score'],
                        'volume_24h': opp["pool_data"].get("volume_24h", 0),
                        'volume_6h': opp["pool_data"].get("volume_6h", 0),
                        'volume_1h': opp["pool_data"].get("volume_1h", 0),
                        'liquidity': opp["pool_data"].get("liquidity", 0),
                        'buys_24h': opp["pool_data"].get("buys_24h", 0),
                        'sells_24h': opp["pool_data"].get("sells_24h", 0),
                        'buy_ratio': opp["pool_data"].get("buy_ratio", 0),
                        'total_txns': opp["pool_data"].get("total_txns", 0),
                        'age_hours': opp["pool_data"].get("age_hours", 0),
                        'volume_acceleration_1h_vs_6h': opp["pool_data"].get("volume_acceleration_1h_vs_6h", 0),
                        'volume_acceleration_6h_vs_24h': opp["pool_data"].get("volume_acceleration_6h_vs_24h", 0),
                        'entry_price': entry_price,
                        'stop_loss_price': stop_loss_price,
                        'stop_loss_percent': -10,
                        'tp1_price': tp1_price,
                        'tp1_percent': 5,
                        'tp2_price': tp2_price,
                        'tp2_percent': 10,
                        'tp3_price': tp3_price,
                        'tp3_percent': 15,
                        'alert_message': alert_msg,
                        # R√àGLE 5: Donn√©es de v√©locit√© du pump
                        'velocite_pump': regle5_data['velocite_pump'],
                        'type_pump': regle5_data['type_pump'],
                        'decision_tp_tracking': regle5_data['decision_tp_tracking'],
                        'temps_depuis_alerte_precedente': regle5_data['temps_depuis_alerte_precedente'],
                        'is_alerte_suivante': regle5_data['is_alerte_suivante']
                    }

                    alert_id = alert_tracker.save_alert(alert_data)
                    if alert_id > 0:
                        log(f"   üíæ Sauvegard√© en DB (ID: {alert_id}) - Tracking auto d√©marr√©")
                    else:
                        log(f"   ‚ö†Ô∏è √âchec sauvegarde DB (token d√©j√† existant?)")

                except Exception as e:
                    log(f"   ‚ö†Ô∏è Erreur sauvegarde DB: {e}")

                alerts_sent += 1
            else:
                log(f"‚ùå √âchec alerte: {opp['pool_data']['name']}")

            if alerts_sent >= MAX_ALERTS_PER_SCAN:
                log(f"‚ö†Ô∏è Limite {MAX_ALERTS_PER_SCAN} alertes atteinte")
                break

            time.sleep(1)
        else:
            # Cooldown actif - alerte bloqu√©e (ne devrait jamais arriver avec COOLDOWN_SECONDS = 0)
            log(f"‚è∞ Alerte bloqu√©e (cooldown actif): {opp['pool_data']['name']}")

    # ==========================================
    # TRACKING ACTIF DES ALERTES (BACKTESTING)
    # ==========================================
    if ENABLE_ACTIVE_TRACKING and alert_tracker is not None:
        log(f"\nüì° TRACKING ACTIF: V√©rification des pools alert√©s...")

        active_alerts = alert_tracker.get_active_alerts(max_age_hours=ACTIVE_TRACKING_MAX_AGE_HOURS)
        log(f"   üîç {len(active_alerts)} alertes actives √† tracker (< {ACTIVE_TRACKING_MAX_AGE_HOURS}h)")

        updates_sent = 0
        for alert in active_alerts:
            try:
                alert_id = alert['id']
                token_name = alert['token_name']
                pool_address = alert['token_address']
                network = alert['network']
                created_at_str = alert['created_at']

                # V√©rifier cooldown (√©viter spam)
                from datetime import datetime
                created_at = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))
                now = datetime.now(created_at.tzinfo) if created_at.tzinfo else datetime.now()
                minutes_elapsed = (now - created_at).total_seconds() / 60

                # V√©rifier si derni√®re mise √† jour √©tait il y a moins de COOLDOWN minutes
                # Pour simplifier, on consid√®re que si l'alerte a moins de COOLDOWN minutes, on skip
                if minutes_elapsed < ACTIVE_TRACKING_UPDATE_COOLDOWN_MINUTES:
                    continue  # Trop r√©cent, skip

                # R√©cup√©rer donn√©es actuelles du pool
                pool_data = get_pool_by_address(network, pool_address)

                if not pool_data or not isinstance(pool_data, dict):
                    # Pool plus disponible (delisted, erreur API, etc.)
                    log(f"   ‚ö†Ô∏è Pool data invalide pour {token_name}: {type(pool_data)}")
                    continue

                current_price = pool_data.get('price_usd', 0)

                if current_price <= 0:
                    continue

                # Mettre √† jour le prix MAX en temps r√©el
                alert_tracker.update_price_max_realtime(alert_id, current_price)

                # V√©rifier si on doit envoyer une mise √† jour Telegram
                should_send, reason = should_send_alert(pool_address, current_price, alert_tracker, None)

                if should_send:
                    log(f"   üîÑ Mise √† jour: {token_name} - {reason}")

                    # R√©cup√©rer momentum et multi-pool (optionnel pour mises √† jour)
                    momentum = get_price_momentum_from_api(pool_data)
                    multi_pool_data = {}  # Optionnel pour updates

                    # Calculer score et whale analysis
                    score, base_score, momentum_bonus, whale_analysis = calculate_final_score(pool_data, momentum, multi_pool_data)

                    # G√©n√©rer message d'alerte (is_first_alert = False)
                    try:
                        alert_msg, regle5_data = generer_alerte_complete(
                            pool_data, score, base_score, momentum_bonus, momentum,
                            multi_pool_data, [], None, whale_analysis,
                            is_first_alert=False,  # C'est une mise √† jour
                            tracker=alert_tracker
                        )
                    except Exception as gen_error:
                        log(f"   ‚ùå Erreur g√©n√©ration alerte pour {token_name}: {gen_error}")
                        import traceback
                        log(f"   Traceback: {traceback.format_exc()}")
                        continue  # Skip cette alerte

                    # Envoyer via Telegram
                    success = send_telegram(alert_msg)

                    if success:
                        updates_sent += 1
                        log(f"   ‚úÖ Mise √† jour envoy√©e pour {token_name}")

                        # Limiter le nombre de mises √† jour par scan
                        if updates_sent >= 5:  # Max 5 mises √† jour par scan
                            log(f"   ‚ö†Ô∏è Limite 5 mises √† jour atteinte")
                            break
                    else:
                        log(f"   ‚ùå √âchec envoi mise √† jour: {token_name}")

                    time.sleep(1)  # Pause entre mises √† jour

            except Exception as e:
                log(f"   ‚ùå Erreur tracking {alert.get('token_name', 'unknown')}: {e}")

        log(f"   üìä Tracking termin√©: {updates_sent} mises √† jour envoy√©es")

    log(f"\n‚úÖ Scan termin√©: {alerts_sent} alertes envoy√©es, {tokens_rejected} tokens rejet√©s (s√©curit√©)")
    log("=" * 80)

# ============================================
# MAIN
# ============================================
def main():
    """Boucle principale."""
    global security_checker, alert_tracker

    log("üöÄ D√©marrage GeckoTerminal Scanner V3...")
    log(f"üì° R√©seaux surveill√©s: {', '.join([n.upper() for n in NETWORKS])}")
    log(f"üìã Seuils par r√©seau (liq/vol/txns):")
    log(f"   ‚Ä¢ Solana/BSC/ETH/Base: $100K / $50K / 100 txns")
    log(f"   ‚Ä¢ Arbitrum: $2K / $400 / 10 txns")
    log(f"‚è∞ Age max: {MAX_TOKEN_AGE_HOURS}h")
    log(f"üîÑ Scan toutes les 5 minutes")
    log(f"üéØ Max {MAX_ALERTS_PER_SCAN} alertes par scan")

    # Initialiser le syst√®me de s√©curit√© et tracking
    log("\nüîí Initialisation du syst√®me de s√©curit√©...")
    security_checker = SecurityChecker()

    # Chemin DB : volume persistant Railway (/data) ou local
    db_path = os.getenv("DB_PATH", "/data/alerts_history.db")
    alert_tracker = AlertTracker(db_path=db_path)
    log(f"üíæ Base de donn√©es: {db_path}")

    log("‚úÖ Syst√®me de s√©curit√© activ√©")

    while True:
        try:
            scan_geckoterminal()

            log("\nüí§ Pause 5 min avant prochain scan...\n")
            time.sleep(300)

        except KeyboardInterrupt:
            log("\n‚èπÔ∏è  Arr√™t du scanner")
            break

        except Exception as e:
            log(f"‚ùå Erreur: {e}")
            import traceback
            traceback.print_exc()
            log("‚è≥ Pause 60s avant retry...")
            time.sleep(60)

    # Fermer proprement les connexions
    if alert_tracker:
        log("üîí Fermeture de la base de donn√©es...")
        alert_tracker.close()
        log("‚úÖ Base de donn√©es ferm√©e")

if __name__ == "__main__":
    main()
